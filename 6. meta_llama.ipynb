{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run an open-source quantized LLAMA2 model, a few steps are needed first. Open up your terminal and run the following commands:\n",
    "\n",
    "1. sudo apt-get update\n",
    "2. sudo apt-get install build-essential\n",
    "3. export CC=/usr/bin/gcc\n",
    "4. export CXX=/usr/bin/g++\n",
    "5. CMAKE_ARGS=\"-DLLAMA_CULABS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.7 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "6. pip install huggingface_hub\n",
    "7. pip install llama-cpp-python==0.1.78\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'TheBloke/Llama-2-13B-chat-GGML'\n",
    "model_basename = 'llama-2-13b-chat.ggmlv3.q5_1.bin' # The model in bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(\n",
    "    repo_id = model_name_or_path,\n",
    "    filename = model_basename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anubrata/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/anubrata/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# To run on GPU\n",
    "lcpp_llm = None\n",
    "\n",
    "lcpp_llm = Llama(\n",
    "    model_path = model_path,\n",
    "    n_threads = 2, # No. of CPU cores\n",
    "    n_batch = 512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU\n",
    "    n_gpu_layers = 32 # Change this value based on your model and your GPU VRAM pool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the number of layers in GPU\n",
    "lcpp_llm.params.n_gpu_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a linear regression code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = f\"\"\"\n",
    "    SYSTEM: You are a helpful, respectful and an honest assistant. Always answer to the best of your ability.\n",
    "    \n",
    "    USER: {prompt}\n",
    "    \n",
    "    ASSISTANT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 21496.90 ms\n",
      "llama_print_timings:      sample time =   128.27 ms /   256 runs   (    0.50 ms per token,  1995.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6958.11 ms /    15 tokens (  463.87 ms per token,     2.16 tokens per second)\n",
      "llama_print_timings:        eval time = 117123.75 ms /   255 runs   (  459.31 ms per token,     2.18 tokens per second)\n",
      "llama_print_timings:       total time = 124707.93 ms\n"
     ]
    }
   ],
   "source": [
    "response = lcpp_llm(\n",
    "    prompt = prompt_template,\n",
    "    max_tokens = 256,\n",
    "    temperature = 0.5,\n",
    "    top_p = 0.95,\n",
    "    repeat_penalty = 1.2,\n",
    "    top_k = 150,\n",
    "    echo = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-0146d0ce-f865-4ad5-8e75-8dcabe8e58b5',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1720969375,\n",
       " 'model': '/home/anubrata/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin',\n",
       " 'choices': [{'text': \"\\n    SYSTEM: You are a helpful, respectful and an honest assistant. Always answer to the best of your ability.\\n    \\n    USER: Write a linear regression code\\n    \\n    ASSISTANT:\\n        Sure! Here's an example of how you could write a simple linear regression code in Python using scikit-learn library:\\n```\\nimport pandas as pd\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.models import Model\\n\\n# Load the data\\ndf = pd.read_csv('data.csv')\\n\\n# Create an X and y matrix from the data\\nX = df[['feature1', 'feature2']]\\ny = df['target']\\n\\n# Split the data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\\n\\n# Create a linear regression object with default parameters\\nreg = LinearRegression()\\n\\n# Train the model on the training data\\nreg.fit(X_train, y_train)\\n\\n# Make predictions on the testing data\\ny_pred = reg.predict(X_test)\\n\\n# Evaluate the performance of the model using mean squared error (MSE)\\nmse = np.mean((y_test - y_pred) ** 2)\\n\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 51, 'completion_tokens': 256, 'total_tokens': 307}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SYSTEM: You are a helpful, respectful and an honest assistant. Always answer to the best of your ability.\n",
      "    \n",
      "    USER: Write a linear regression code\n",
      "    \n",
      "    ASSISTANT:\n",
      "        Sure! Here's an example of how you could write a simple linear regression code in Python using scikit-learn library:\n",
      "```\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.models import Model\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Create an X and y matrix from the data\n",
      "X = df[['feature1', 'feature2']]\n",
      "y = df['target']\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "\n",
      "# Create a linear regression object with default parameters\n",
      "reg = LinearRegression()\n",
      "\n",
      "# Train the model on the training data\n",
      "reg.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions on the testing data\n",
      "y_pred = reg.predict(X_test)\n",
      "\n",
      "# Evaluate the performance of the model using mean squared error (MSE)\n",
      "mse = np.mean((y_test - y_pred) ** 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testingopenai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
