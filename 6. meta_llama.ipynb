{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Open-Source Llama Model without Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run an open-source quantized LLAMA2 model, a few steps are needed first. Open up your terminal and run the following commands:\n",
    "\n",
    "1. sudo apt-get update\n",
    "2. sudo apt-get install build-essential\n",
    "3. export CC=/usr/bin/gcc\n",
    "4. export CXX=/usr/bin/g++\n",
    "5. CMAKE_ARGS=\"-DLLAMA_CULABS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.7 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "6. pip install huggingface_hub\n",
    "7. pip install llama-cpp-python==0.1.78\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'TheBloke/Llama-2-13B-chat-GGML'\n",
    "model_basename = 'llama-2-13b-chat.ggmlv3.q5_1.bin' # The model in bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(\n",
    "    repo_id = model_name_or_path,\n",
    "    filename = model_basename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anubrata/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/anubrata/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# To run on GPU\n",
    "lcpp_llm = None\n",
    "\n",
    "lcpp_llm = Llama(\n",
    "    model_path = model_path,\n",
    "    n_threads = 2, # No. of CPU cores\n",
    "    n_batch = 512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU\n",
    "    n_gpu_layers = 32 # Change this value based on your model and your GPU VRAM pool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the number of layers in GPU\n",
    "lcpp_llm.params.n_gpu_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a linear regression code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = f\"\"\"\n",
    "    SYSTEM: You are a helpful, respectful and an honest assistant. Always answer to the best of your ability.\n",
    "    \n",
    "    USER: {prompt}\n",
    "    \n",
    "    ASSISTANT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 21496.90 ms\n",
      "llama_print_timings:      sample time =   128.27 ms /   256 runs   (    0.50 ms per token,  1995.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6958.11 ms /    15 tokens (  463.87 ms per token,     2.16 tokens per second)\n",
      "llama_print_timings:        eval time = 117123.75 ms /   255 runs   (  459.31 ms per token,     2.18 tokens per second)\n",
      "llama_print_timings:       total time = 124707.93 ms\n"
     ]
    }
   ],
   "source": [
    "response = lcpp_llm(\n",
    "    prompt = prompt_template,\n",
    "    max_tokens = 256,\n",
    "    temperature = 0.5,\n",
    "    top_p = 0.95,\n",
    "    repeat_penalty = 1.2,\n",
    "    top_k = 150,\n",
    "    echo = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-0146d0ce-f865-4ad5-8e75-8dcabe8e58b5',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1720969375,\n",
       " 'model': '/home/anubrata/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin',\n",
       " 'choices': [{'text': \"\\n    SYSTEM: You are a helpful, respectful and an honest assistant. Always answer to the best of your ability.\\n    \\n    USER: Write a linear regression code\\n    \\n    ASSISTANT:\\n        Sure! Here's an example of how you could write a simple linear regression code in Python using scikit-learn library:\\n```\\nimport pandas as pd\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.models import Model\\n\\n# Load the data\\ndf = pd.read_csv('data.csv')\\n\\n# Create an X and y matrix from the data\\nX = df[['feature1', 'feature2']]\\ny = df['target']\\n\\n# Split the data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\\n\\n# Create a linear regression object with default parameters\\nreg = LinearRegression()\\n\\n# Train the model on the training data\\nreg.fit(X_train, y_train)\\n\\n# Make predictions on the testing data\\ny_pred = reg.predict(X_test)\\n\\n# Evaluate the performance of the model using mean squared error (MSE)\\nmse = np.mean((y_test - y_pred) ** 2)\\n\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 51, 'completion_tokens': 256, 'total_tokens': 307}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SYSTEM: You are a helpful, respectful and an honest assistant. Always answer to the best of your ability.\n",
      "    \n",
      "    USER: Write a linear regression code\n",
      "    \n",
      "    ASSISTANT:\n",
      "        Sure! Here's an example of how you could write a simple linear regression code in Python using scikit-learn library:\n",
      "```\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.models import Model\n",
      "\n",
      "# Load the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Create an X and y matrix from the data\n",
      "X = df[['feature1', 'feature2']]\n",
      "y = df['target']\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "\n",
      "# Create a linear regression object with default parameters\n",
      "reg = LinearRegression()\n",
      "\n",
      "# Train the model on the training data\n",
      "reg.fit(X_train, y_train)\n",
      "\n",
      "# Make predictions on the testing data\n",
      "y_pred = reg.predict(X_test)\n",
      "\n",
      "# Evaluate the performance of the model using mean squared error (MSE)\n",
      "mse = np.mean((y_test - y_pred) ** 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Open-Source Llama Model using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "load_dotenv()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd481b29939f4adab454f0218a701954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 7b parameter model from meta, you need to get access first. If you don't have it, run the daryll_model\n",
    "# eta_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "daryll_model = \"daryl149/llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(daryll_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ee7467473841628f49580969d55b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fc5c69d9a543be96efcfb4ca08e57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   4%|4         | 430M/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1080eab4da470cb0ed27de2585b888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4bccced521445bb7a8e5db2300e33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2cd6fe955e40dd837e747d934e33b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model = daryll_model,\n",
    "    tokenizer = tokenizer,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    trust_remote_code = True,\n",
    "    device_map = \"auto\",\n",
    "    max_length = 1000,\n",
    "    do_sample = True,\n",
    "    top_k = 10,\n",
    "    num_return_sequences = 1,\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_hf_model = HuggingFacePipeline(\n",
    "    pipeline = pipeline,\n",
    "    model_kwargs = {\n",
    "        'temperature': 0 # No need for creativity\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What do you know about Arsenal FC?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you know about Arsenal FC?\n",
      "\n",
      "Arsenal Football Club is an English professional football club based in Islington, London, that plays in the Premier League, the top flight of English football. The club was founded in 1886 as Dial Square and changed its name to Royal Arsenal in 1892, before being renamed as Arsenal in 1913.\n",
      "Arsenal has a long and successful history, having won 13 league titles, a record 13 FA Cups, two League Cups, and 16 FA Community Shields. The club has also enjoyed success in European competitions, winning the European Cup Winners' Cup in 1994 and the UEFA Cup Winners' Cup in 1993.\n",
      "Arsenal plays its home games at the Emirates Stadium, which opened in 2006 and has a seating capacity of over 60,000 spectators. The club's traditional colors are red and white, and its crest features a cannon and a bow and arrow, symbolizing the club's nickname, \"The Gunners.\"\n",
      "Arsenal has a large and dedicated fan base, with supporters clubs around the world. The club has a rivalry with neighbors Tottenham Hotspur, with whom it contests the North London Derby, as well as with Chelsea and Manchester United in the Premier League.\n",
      "Arsenal's most successful period was in the early 2000s, under the management of Arsène Wenger, who led the club to three Premier League titles and four FA Cups between 1997 and 2018. The club has also produced a number of talented players over the years, including Thierry Henry, Dennis Bergkamp, Patrick Vieira, and Cesc Fàbregas.\n",
      "In summary, Arsenal FC is a historic and successful English football club with a rich tradition of success in domestic and European competitions. The club has a large and dedicated fan base, a state-of-the-art stadium, and a proud nickname of \"The Gunners.\"\n"
     ]
    }
   ],
   "source": [
    "print(meta_hf_model(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you know about Barcelona FC?\n"
     ]
    }
   ],
   "source": [
    "# Using Langchain Prompt Templates\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['football_club'],\n",
    "    template=\"What do you know about {football_club} FC?\"\n",
    ")\n",
    "\n",
    "input_prompt = prompt_template.format(football_club = \"Barcelona\")\n",
    "print(input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat do you know about Barcelona FC?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "What do you know about Barcelona FC? Barcelona FC, also known as Barça, is a professional football club based in Barcelona, Spain. They are one of the most successful clubs in Spanish football history and have won numerous domestic and international titles. Here are some key facts about Barcelona FC:\n",
      "History: Barcelona FC was founded in 1899 by a group of football enthusiasts led by Joan Gamper. The club has a rich history and has won 26 La Liga titles, 14 Copa del Rey trophies, and 5 UEFA Champions League titles.\n",
      "Playing style: Barcelona has a distinct playing style that focuses on possession football, quick passing, and creative attacking play. Their philosophy emphasizes teamwork, creativity, and individual skill.\n",
      "Home ground: Barcelona plays their home matches at the Camp Nou stadium, which has a seating capacity of over 99,000 spectators. It is one of the largest stadiums in Europe and has hosted numerous international matches, as well as concerts and other sporting events.\n",
      "Legendary players: Barcelona has produced some of the greatest footballers in history, including Lionel Messi, Xavi Hernandez, Andres Iniesta, and Gerard Pique. These players have helped establish the club as a leader in Spanish and European football.\n",
      "Rivalries: Barcelona has several rivalries with other Spanish clubs, including Real Madrid, Atlético Madrid, and Valencia CF. These rivalries are fueled by the clubs' geographical proximity and their long history of competition.\n",
      "Fan culture: Barcelona has a passionate fan base with a rich culture of singing, chanting, and supporting the team. The club's supporters are known for their dedication and loyalty, and they are considered to be among the best in the world.\n",
      "Business ventures: Barcelona FC has expanded its reach beyond football through various business ventures, including a chain of restaurants, a fashion label, and a sports media company.\n",
      "Barcelona FC is a beloved institution in Catalonia and Spain, and its legacy continues to inspire football fans around the world. Whether you're a die-hard supporter or just a casual fan, understanding the club's history, playing style, and cultural significance can enhance your appreciation for the beautiful game of football.\n"
     ]
    }
   ],
   "source": [
    "# Complete the chain and extract the results\n",
    "chain = LLMChain(\n",
    "    llm = meta_hf_model,\n",
    "    prompt = prompt_template,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "response = chain.run(\"Barcelona\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testingopenai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
