{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.agents import AgentType, load_tools, initialize_agent\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Langchain Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "As of 2021, there are 48 countries in Asia. The top 10 countries in Asia based on population are:\n",
      "\n",
      "1. China\n",
      "2. India\n",
      "3. Indonesia\n",
      "4. Pakistan\n",
      "5. Bangladesh\n",
      "6. Japan\n",
      "7. Philippines\n",
      "8. Vietnam\n",
      "9. Iran\n",
      "10. Turkey\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you tell me the total number of countries in Asia? Can you name the top 10 based on population\"\n",
    "print(client.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st way \n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['country'],\n",
    "    template = 'Can you tell me the capital of {country}?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = prompt_template_name.format(country = 'India')\n",
    "prompt2 = prompt_template_name.format(country = 'Romania')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Romania is Bucharest.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt2).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd way\n",
    "prompt_template_name_2 = PromptTemplate.from_template(\"What is a good name for a company that makes {products}?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = prompt_template_name_2.format(products = 'toys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Playful Creations Co.\n",
      "2. Toytopia Inc.\n",
      "3. WonderWorks Toys\n",
      "4. Imagination Factory\n",
      "5. Funland Toys\n",
      "6. Happyland Playthings\n",
      "7. Toylandia Enterprises\n",
      "8. Creativity Kids Co.\n",
      "9. Joyful Jax Toys\n",
      "10. Whimsy World Inc.\n",
      "11. Dreamland Toys\n",
      "12. Magic Makers Co.\n",
      "13. The Toy Emporium\n",
      "14. Playtime Innovations\n",
      "15. Toy Kingdom Co.\n",
      "16. Imagine and Play Co.\n",
      "17. Sunny Side Toys\n",
      "18. SparklePlay Co.\n",
      "19. Toytopolis Inc.\n",
      "20. WonderWorks Toy Co.\n"
     ]
    }
   ],
   "source": [
    "print(client.predict(prompt3).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first see what's the problem\n",
    "\n",
    "prompt4 = \"Can you tell me who won the recent football world cup?\"\n",
    "prompt5 = \"Can you tell me the current GDP of India?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recent football world cup, also known as the 2018 FIFA World Cup, was won by France. They defeated Croatia 4-2 in the final match.\n"
     ]
    }
   ],
   "source": [
    "print(client.predict(prompt4).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2021, the current GDP of India is approximately $3.05 trillion USD.\n"
     ]
    }
   ],
   "source": [
    "print(client.predict(prompt5).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information we get is dependent on the last trained data, and hence, is not recent. In order to overcome the problem, we use Agents who can scrape the data\n",
    "from the internet using APIs.\n",
    "For extracting real-time information, we will be using SerpAI ( pip install google-search-results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = load_tools([\"serpapi\"], llm=client)\n",
    "agent = initialize_agent(tool, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should try searching for the answer.\n",
      "Action: Search\n",
      "Action Input: \"who won the recent football world cup\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'title': '2022 World Cup', 'games': [{'tournament': 'World Cup', 'stage': 'Final', 'stadium': 'Lusail Stadium', 'stadium_kgmid': '/m/0fq1625', 'status': 'FT (P)', 'date': 'Dec 18, 22', 'video_highlights': {'link': 'https://www.plus.fifa.com/content/46772b92-24c3-4315-87da-158ef7af6259?cid=(p_go-box)(c_FIFAPlus)(sc_highlights)(ssc_)(da_)(co_)(cc_)&s_cid=(co_)(cc_)&c=(c_FIFAPlus)(sc_highlights)(ssc_)(cc_)&pid=(p_go-box)(ch_)(t_)', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ced1fd99cf3b9d23cce91be909e40f404f53e0014f444dd676ceb9c91be4f28304b741d23dc93aba10.jpeg', 'duration': '2:0'}, 'teams': [{'name': 'Argentina', 'score': '3 (4)', 'kgmid': '/m/02bh_v', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93cea6fe1a12c9c3b675143440a3079511491dfd15d7856f474e76960f279a0b7b32.png'}, {'name': 'France', 'score': '3 (2)', 'kgmid': '/m/01l3vx', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93cea6fe1a12c9c3b675c75c31e0562f4b568e9b92fa45cfa9d81d614dc20bf51643.png'}]}, {'tournament': 'World Cup', 'stage': 'Third place play-off', 'stadium': 'Khalifa International Stadium', 'stadium_kgmid': '/m/09s88f', 'status': 'FT', 'date': 'Dec 17, 22', 'video_highlights': {'link': 'https://www.plus.fifa.com/content/09e75c23-ec7d-4a6c-8b67-786e8c8c2627?cid=(p_go-box)(c_FIFAPlus)(sc_highlights)(ssc_)(da_)(co_)(cc_)&s_cid=(co_)(cc_)&c=(c_FIFAPlus)(sc_highlights)(ssc_)(cc_)&pid=(p_go-box)(ch_)(t_)', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ce145c23aba996306728d8298af87367fda0f1a07daf9be1032e6bab87a922009c9a6b48e9cca03835.jpeg', 'duration': '2:0'}, 'teams': [{'name': 'Croatia', 'score': '2', 'kgmid': '/m/0356gk', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ce237f090e37b6e1b53eb2a3c42fd7dc1740de38efec7a4b7a14f5ff612638d2bd.png'}, {'name': 'Morocco', 'score': '1', 'kgmid': '/m/03_9hm', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ce237f090e37b6e1b5ab2e5d7906b19b5e2de9bc8674489bad334f16454980314b.png'}]}, {'tournament': 'World Cup', 'stage': 'Semi-finals', 'stadium': 'Al Bayt Stadium', 'stadium_kgmid': '/m/0fqsmsx', 'status': 'FT', 'date': 'Dec 14, 22', 'video_highlights': {'link': 'https://www.plus.fifa.com/content/7e2abcd7-4379-4bdd-95d6-a45ef0d977b0?cid=(p_go-box)(c_FIFAPlus)(sc_highlights)(ssc_)(da_)(co_)(cc_)&s_cid=(co_)(cc_)&c=(c_FIFAPlus)(sc_highlights)(ssc_)(cc_)&pid=(p_go-box)(ch_)(t_)', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93cef7c8bc612b8dce9034c879b20e1b0a97427edec815b5ddf2afe53cb49b1237b2d4da7bceab3d4a9a.jpeg', 'duration': '2:0'}, 'teams': [{'name': 'France', 'score': '2', 'kgmid': '/m/01l3vx', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ce640009a726f91d42a5953616b53cfe147bd77b226b7300f449d6b19bbed682db.png'}, {'name': 'Morocco', 'score': '0', 'kgmid': '/m/03_9hm', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ce640009a726f91d42717c91a8707fe6344fb07f8e349053b17e802578a856345c.png'}]}, {'tournament': 'World Cup', 'stage': 'Semi-finals', 'stadium': 'Lusail Stadium', 'stadium_kgmid': '/m/0fq1625', 'status': 'FT', 'date': 'Dec 13, 22', 'video_highlights': {'link': 'https://www.plus.fifa.com/content/9b1396af-643b-4613-a455-a4aa4339314a?cid=(p_go-box)(c_FIFAPlus)(sc_highlights)(ssc_)(da_)(co_)(cc_)&s_cid=(co_)(cc_)&c=(c_FIFAPlus)(sc_highlights)(ssc_)(cc_)&pid=(p_go-box)(ch_)(t_)', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93cede0ec5305a16ef249ab6082922af0910e5fab0750e7b52c1c20337a37da869fea10a1b9e42698b42.jpeg', 'duration': '2:0'}, 'teams': [{'name': 'Argentina', 'score': '3', 'kgmid': '/m/02bh_v', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ce0b48092ffaaa62d51d9ea1960e9e7d3862d31e19d5de1be308158995d5195257.png'}, {'name': 'Croatia', 'score': '0', 'kgmid': '/m/0356gk', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ce0b48092ffaaa62d50c61c636017f1587fb42f50917bdce4ea686ad060d84b017.png'}]}, {'tournament': 'World Cup', 'stage': 'Quarter-finals', 'stadium': 'Al Bayt Stadium', 'stadium_kgmid': '/m/0fqsmsx', 'status': 'FT', 'date': 'Dec 10, 22', 'video_highlights': {'link': 'https://www.plus.fifa.com/content/c8c095cd-9f14-4c4c-bc0f-50f9c3659fec?cid=(p_go-box)(c_FIFAPlus)(sc_highlights)(ssc_)(da_)(co_)(cc_)&s_cid=(co_)(cc_)&c=(c_FIFAPlus)(sc_highlights)(ssc_)(cc_)&pid=(p_go-box)(ch_)(t_)', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93cebe5b15268a7d12772af50286ba2d47a0f470c068960cfc1729104d67959208d46fe65c71cd028210.jpeg', 'duration': '2:0'}, 'teams': [{'name': 'England', 'score': '1', 'kgmid': '/m/02pp1', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ce38a569f64758c759549b622fe69ed0ca85641500ae465dd5828fbada3284f3f0.png'}, {'name': 'France', 'score': '2', 'kgmid': '/m/01l3vx', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ce38a569f64758c759eb2bd100668787a77d68ee328cdc2e5f94bca326dabbcffb.png'}]}, {'tournament': 'World Cup', 'stage': 'Quarter-finals', 'stadium': 'Al Thumama Stadium', 'stadium_kgmid': '/g/11f06l125x', 'status': 'FT', 'date': 'Dec 10, 22', 'video_highlights': {'link': 'https://www.plus.fifa.com/content/0b79ae56-6ae4-4d62-b18e-02050be9f6d4?cid=(p_go-box)(c_FIFAPlus)(sc_highlights)(ssc_)(da_)(co_)(cc_)&s_cid=(co_)(cc_)&c=(c_FIFAPlus)(sc_highlights)(ssc_)(cc_)&pid=(p_go-box)(ch_)(t_)', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93ce02d1c3d4f9e71fb989bb776dcc1631266e3ff076bee803b4d97e81b685b8cf87a08782a0797aeea0.jpeg', 'duration': '2:0'}, 'teams': [{'name': 'Morocco', 'score': '1', 'red_card': True, 'kgmid': '/m/03_9hm', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93cede0361290992e56ca05804d546b43598f2417c7dbae0a8d4d682596c1d02cd0b.png'}, {'name': 'Portugal', 'score': '0', 'kgmid': '/m/02rqxc', 'thumbnail': 'https://serpapi.com/searches/668ec377ef7dda8ba5c1e252/images/405aab89c4c1d87d5cc981426bdd93cede0361290992e56c10fbdd7ff87d966a181e558a5a6d5899a038449271e6f913.png'}]}]}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should refine my search to just the winner of the final game.\n",
      "Action: Search\n",
      "Action Input: \"winner of 2022 football world cup\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mArgentina national football team\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This is not the result I was looking for. I should try a different search query.\n",
      "Action: Search\n",
      "Action Input: \"winner of 2022 football world cup final\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mArgentina national football team\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This is still not the result I was looking for. I should try another search query.\n",
      "Action: Search\n",
      "Action Input: \"who won the 2022 football world cup\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mArgentina national football team\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This is not the correct answer. I need to try one more search query.\n",
      "Action: Search\n",
      "Action Input: \"who won the 2022 football world cup final\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mArgentina national football team\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Argentina national football team\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Argentina national football team'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Can you tell me who won the recent football world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should think about using a search engine to find this information\n",
      "Action: Search\n",
      "Action Input: \"Current GDP India\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[\"India, officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area; the most populous country as of June 2023; and from the time of its independence in 1947, the world's most populous democracy.\", 'India type: Country in South Asia.', 'India entity_type: countries, locations, travel, competitions, military, travel.', 'India kgmid: /m/03rk0.', 'India capital: New Delhi.', 'India co2_emissions_per_capita: 1.58 metric tons (2020).', 'India electricity_consumption_per_capita: 797.35 kWh (2014).', 'India energy_use_per_capita: 630.90 kg of oil equivalent (2014).', 'India fertility_rate: 2.03 births per woman (2021).', 'India gdp_growth_rate: 7.2% annual change (2022).', 'India gross_domestic_product: 3.417 trillion USD (2022).', 'Economy of India ; Increase $3.937 trillion (nominal; 2024 est.) · Increase $14.59 trillion (PPP; 2024 est.) · 5th (nominal; 2024) · 3rd (PPP; 2024) · Increase 8.2% ...', 'In 2023, GDP in India was at around 3.57 trillion U.S. dollars, and it is expected to reach six trillion by the end of the decade. See figures ...', 'GDP per capita, current prices. U.S. dollars per capita. India. GDP, current prices. Purchasing power parity; billions of international dollars. 14.59.', 'GDP (current US$) - India from The World Bank: Data.', 'Nominal (current) Gross Domestic Product (GDP) of India is $3,385,090,000,000 (USD) as of 2022. · Real GDP (constant, inflation adjusted) of India reached ...', \"India's Current GDP (As of Q3 FY24): $4.11 trillion · India's GDP Growth Rate (Q3 FY24): 8.4%.\", 'The Gross Domestic Product (GDP) in India was worth 3549.92 billion US dollars in 2023, according to official data from the World Bank. The GDP value of ...', 'India. 3.94 thousand · United Kingdom. 3.5 thousand.', 'Nominal GDP or GDP at Current Prices in Q4 of 2023-24 is estimated at ₹78.28 lakh crore, against ₹71.23 lakh crore in Q4 of 2022-23, showing a ...', 'Nominal GDP or GDP at Current Prices is estimated to attain a level of ₹295.36 lakh crore in the year 2023-24, against ₹269.50 lakh crore ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The current GDP of India is approximately $3.94 trillion USD (2023).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current GDP of India is approximately $3.94 trillion USD (2023).'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Can you tell me the current GDP of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create another tool with wikipedia(!pip install wikipedia)\n",
    "wiki_tool = load_tools(['wikipedia'], llm=client)\n",
    "wiki_agent = initialize_agent(wiki_tool, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m It would be helpful to check Wikipedia for information on Euro 2024.\n",
      "Action: wikipedia\n",
      "Action Input: Euro 2024\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: UEFA Euro 2024\n",
      "Summary: The 2024 UEFA European Football Championship, commonly referred to as UEFA Euro 2024 (stylised as UEFA EURO 2024), or simply Euro 2024, is the ongoing 17th edition of the UEFA European Championship, the quadrennial international football  championship organised by UEFA for the European men's national teams of its member associations. Germany is hosting the tournament, which is taking place from 14 June to 14 July 2024. The tournament comprises 24 teams, with Georgia making their European Championship debut. \n",
      "It is the third time that European Championship matches are played on German territory and the second time in reunified Germany, as West Germany hosted the tournament's 1988 edition, and four matches of the multi-national Euro 2020 were played in Munich. It is the first time the competition is held in what was formerly East Germany, with Leipzig as a host city, as well as the first major tournament since the 2006 FIFA World Cup that Germany serves as a solo host nation. The tournament returned to its usual four-year cycle after the 2020 edition was postponed to 2021 due to the COVID-19 pandemic.\n",
      "Italy were the defending champions, having won the 2020 final against England on penalties, but were eliminated in the round of 16 by Switzerland. Host nation Germany were eliminated by Spain in the quarter-finals.\n",
      "\n",
      "Page: UEFA Euro 2024 knockout stage\n",
      "Summary: The knockout stage of UEFA Euro 2024 began on 29 June 2024 with the round of 16 and will end on 14 July 2024 with the final at Olympiastadion in Berlin, Germany.\n",
      "All times listed are Central European Summer Time. (UTC+2)\n",
      "\n",
      "\n",
      "\n",
      "Page: UEFA Euro 2024 qualifying\n",
      "Summary: The UEFA Euro 2024 qualifying tournament was a football competition that was played from March 2023 to March 2024 to determine the 23 UEFA member men's national teams that would join the automatically qualified host team Germany in the UEFA Euro 2024 final tournament. The competition was linked with the 2022–23 UEFA Nations League, which gave countries a secondary route to qualify for the final tournament.\n",
      "A total of 53 UEFA member associations entered the qualifying process. The draw for the qualifying group stage took place at the Festhalle in Frankfurt on 9 October 2022.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m After looking through the Wikipedia pages on Euro 2024, I now know that it is the 17th edition of the UEFA European Championship and will be held in Germany in 2024.\n",
      "Final Answer: Euro 2024 is the 17th edition of the UEFA European Championship, hosted by Germany in 2024. It comprises of 24 teams and is the first major tournament held in Germany since the 2006 FIFA World Cup. The knockout stage began on June 29, 2024 and the final will be held on July 14, 2024 at Olympiastadion in Berlin. The qualifying tournament, which took place from March 2023 to March 2024, determined the 23 teams that would join the host team. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Euro 2024 is the 17th edition of the UEFA European Championship, hosted by Germany in 2024. It comprises of 24 teams and is the first major tournament held in Germany since the 2006 FIFA World Cup. The knockout stage began on June 29, 2024 and the final will be held on July 14, 2024 at Olympiastadion in Berlin. The qualifying tournament, which took place from March 2023 to March 2024, determined the 23 teams that would join the host team.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_agent.run(\"Can you tell me anything about Euro 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Chain\n",
    "\n",
    "Central to Langchain is a vital component called Chains. This forms the core connection between one or more LLMs. In certain sophisticated applications, it becomes necessary to chain LLMs together, either with each other or with other elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x7fa943253fd0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fa942f19f30>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['product'], template='What is a good name for a company that makes {product}')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
    "chain_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['product'], template='What is a good name for a company that makes {product}'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x7fa943253fd0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7fa942f19f30>, openai_api_key=SecretStr('**********'), openai_proxy=''))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=client, prompt=chain_prompt, verbose=True)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is a good name for a company that makes Wine\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Vineyard Vines\" or \"Grapevine Co.\"'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Wine\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Multiple Chains by SimpleSequentialChain\n",
    "prompt_startup_name = PromptTemplate(\n",
    "    input_variables = ['startup_name'],\n",
    "    template = \"I want to start a startup for {startup_name}, suggest me a good name for this!\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=client, prompt=prompt_startup_name)\n",
    "\n",
    "prompt_startup_strategy = PromptTemplate(\n",
    "    input_variables = ['name'],\n",
    "    template = \"Suggest some strategies for {name}\"\n",
    ")\n",
    "\n",
    "strategy_chain = LLMChain(llm=client, prompt=prompt_startup_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "\"IntelliGenius\" \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "1. Market research and targeting: Conduct thorough market research to identify the target audience for IntelliGenius. This will help in understanding their needs, preferences, and behaviors, which can then be used to develop effective marketing strategies.\n",
      "\n",
      "2. Social media presence: Create a strong presence on social media platforms like Facebook, Twitter, and LinkedIn. This will help in reaching out to a larger audience and also engage with potential customers.\n",
      "\n",
      "3. Influencer marketing: Collaborate with influencers in the field of education, technology, and innovation to promote IntelliGenius. This will help in building credibility and trust among the target audience.\n",
      "\n",
      "4. Content marketing: Develop high-quality and informative content such as blogs, articles, videos, and infographics related to education and technology. This will help in establishing IntelliGenius as a thought leader in the industry.\n",
      "\n",
      "5. Partnership and collaborations: Partner with schools, colleges, and other educational institutions to promote IntelliGenius as a valuable tool for students. Collaborate with technology companies to enhance the product and reach a wider audience.\n",
      "\n",
      "6. Email marketing: Use email marketing to reach out to potential customers and keep them updated about new features, offers, and promotions.\n",
      "\n",
      "7. Referral program: Encourage satisfied customers to refer IntelliGenius to their friends\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "1. Market research and targeting: Conduct thorough market research to identify the target audience for IntelliGenius. This will help in understanding their needs, preferences, and behaviors, which can then be used to develop effective marketing strategies.\n",
      "\n",
      "2. Social media presence: Create a strong presence on social media platforms like Facebook, Twitter, and LinkedIn. This will help in reaching out to a larger audience and also engage with potential customers.\n",
      "\n",
      "3. Influencer marketing: Collaborate with influencers in the field of education, technology, and innovation to promote IntelliGenius. This will help in building credibility and trust among the target audience.\n",
      "\n",
      "4. Content marketing: Develop high-quality and informative content such as blogs, articles, videos, and infographics related to education and technology. This will help in establishing IntelliGenius as a thought leader in the industry.\n",
      "\n",
      "5. Partnership and collaborations: Partner with schools, colleges, and other educational institutions to promote IntelliGenius as a valuable tool for students. Collaborate with technology companies to enhance the product and reach a wider audience.\n",
      "\n",
      "6. Email marketing: Use email marketing to reach out to potential customers and keep them updated about new features, offers, and promotions.\n",
      "\n",
      "7. Referral program: Encourage satisfied customers to refer IntelliGenius to their friends\n"
     ]
    }
   ],
   "source": [
    "chain = SimpleSequentialChain(chains=[name_chain, strategy_chain], verbose=True)\n",
    "print(chain.run(\"Artificial Intelligence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace SimpleSequentialChain with SequentialChain\n",
    "prompt_startup_name_seq = PromptTemplate(\n",
    "    input_variables = ['startup_field'],\n",
    "    template = \"I want to start a startup for {startup_field}, suggest me a good name for this!\"\n",
    ")\n",
    "\n",
    "name_chain_seq = LLMChain(llm=client, prompt=prompt_startup_name_seq, output_key=\"startup_name\")\n",
    "\n",
    "prompt_startup_strategy_seq = PromptTemplate(\n",
    "    input_variables = ['startup_name'],\n",
    "    template = \"Suggest some strategies for {startup_name}\"\n",
    ")\n",
    "\n",
    "strategy_chain_seq = LLMChain(llm=client, prompt=prompt_startup_strategy_seq, output_key=\"strategy_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'startup_field': 'Artificial Intelligence',\n",
       " 'startup_name': '\\n\\n1. \"Cognify\"\\n2. \"AIgenius\"\\n3. \"NeuroNexus\"\\n4. \"MindMeld\"\\n5. \"IntelliVerse\"\\n6. \"Cortex Innovations\"\\n7. \"DeepThink\"\\n8. \"Prognosis AI\"\\n9. \"SmartBotics\"\\n10. \"Sentient Solutions\"',\n",
       " 'strategy_items': ' \\n\\n1. \"Cognify\":\\na. Develop a user-friendly interface that simplifies complex cognitive tasks.\\nb. Partner with educational institutions to promote learning and training through the use of Cognify.\\nc. Utilize gamification techniques to make cognitive tasks more engaging and fun.\\nd. Offer personalized training programs based on individual needs and goals.\\ne. Collaborate with businesses to integrate Cognify into their employee training and development programs.\\nf. Conduct research and publish white papers to establish credibility and showcase the benefits of using Cognify.\\n\\n2. \"AIgenius\":\\na. Position AIgenius as a cutting-edge and innovative solution for businesses.\\nb. Showcase case studies and success stories of companies that have implemented AIgenius.\\nc. Employ influencer marketing to establish thought leadership and reach a wider audience.\\nd. Offer demo sessions and free trials to allow potential customers to experience the benefits of AIgenius.\\ne. Utilize targeted advertising to reach businesses in industries with high potential for AI implementation.\\nf. Attend industry events and conferences to network and promote AIgenius to potential clients.\\n\\n3. \"NeuroNexus\":\\na. Partner with healthcare professionals and organizations to promote the use of NeuroNexus for brain health and wellness.\\nb. Offer'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_chain = SequentialChain(\n",
    "    chains=[name_chain_seq, strategy_chain_seq],\n",
    "    input_variables=['startup_field'],\n",
    "    output_variables=['startup_name', 'strategy_items'],\n",
    "    verbose=True)\n",
    "\n",
    "seq_chain({\"startup_field\": \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"BiasFairnessSurvey.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 0}, page_content='A Survey on Bias and Fairness in Machine Learning\\nNINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA,\\nKRISTINA LERMAN, and ARAM GALSTYAN, USC-ISI\\nWith the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting\\nfor fairness has gained significant importance in designing and engineering of such systems. AI systems can be\\nused in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure\\nthat these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently\\nsome work has been developed in traditional machine learning and deep learning that address such challenges in\\ndifferent subdomains. With the commercialization of these systems, researchers are becoming more aware of\\nthe biases that these applications can contain and are attempting to address them. In this survey we investigated\\ndifferent real-world applications that have shown biases in various ways, and we listed different sources of\\nbiases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning\\nresearchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined\\ndifferent domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes\\nin the state-of-the-art methods and ways they have tried to address them. There are still many future directions\\nand solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will\\nmotivate researchers to tackle these issues in the near future by observing existing work in their respective fields.\\nCCS Concepts: •Computing methodologies →Artificial intelligence ;Philosophical/theoretical founda-\\ntions of artificial intelligence ;\\nAdditional Key Words and Phrases: Fairness and Bias in Artificial Intelligence, Machine Learning, Deep\\nLearning, Natural Language Processing, Representation Learning\\n1 INTRODUCTION\\nMachine learning algorithms have penetrated every aspect of our lives. Algorithms make movie\\nrecommendations, suggest products to buy, and who to date. They are increasingly used in high-stakes\\nscenarios such as loans [ 113] and hiring decisions [ 19,39]. There are clear benefits to algorithmic\\ndecision-making; unlike people, machines do not become tired or bored [ 45,119], and can take into\\naccount orders of magnitude more factors than people can. However, like people, algorithms are\\nvulnerable to biases that render their decisions “unfair” [6, 121]. In the context of decision-making,\\nfairness is the absence of any prejudice or favoritism toward an individual or group based on\\ntheir inherent or acquired characteristics . Thus, an unfair algorithm is one whose decisions are\\nskewed toward a particular group of people. A canonical example comes from a tool used by courts\\nin the United States to make pretrial detention and release decisions. The software, Correctional\\nOffender Management Profiling for Alternative Sanctions (COMPAS), measures the risk of a person\\nto recommit another crime. Judges use COMPAS to decide whether to release an offender, or to keep\\nhim or her in prison. An investigation into the software found a bias against African-Americans:1\\n1https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\\nAuthors’ address: USC, Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292\\nThis material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement\\nNo. HR0011890019.arXiv:1908.09635v3  [cs.LG]  25 Jan 2022'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 1}, page_content='2 Mehrabi et al.\\nCOMPAS is more likely to have higher false positive rates for African-American offenders than\\nCaucasian offenders in falsely predicting them to be at a higher risk of recommitting a crime or\\nrecidivism. Similar findings have been made in other areas, such as an AI system that judges beauty\\npageant winners but was biased against darker-skinned contestants,2or facial recognition software in\\ndigital cameras that overpredicts Asians as blinking.3These biased predictions stem from the hidden\\nor neglected biases in data or algorithms.\\nIn this survey we identify two potential sources of unfairness in machine learning outcomes—\\nthose that arise from biases in the data and those that arise from the algorithms. We review research\\ninvestigating how biases in data skew what is learned by machine learning algorithms, and nuances\\nin the way the algorithms themselves work to prevent them from making fair decisions—even when\\nthe data is unbiased. Furthermore, we observe that biased algorithmic outcomes might impact user\\nexperience, thus generating a feedback loop between data, algorithms and users that can perpetuate\\nand even amplify existing sources of bias.\\nWe begin the review with several highly visible real-world cases of where unfair machine learning\\nalgorithms have led to suboptimal and discriminatory outcomes in Section 2. In Section 3, we\\ndescribe the different types and sources of biases that occur within the data-algorithms-users loop\\nmentioned above. Next, in Section 4, we present the different ways that the concept of fairness has\\nbeen operationalized and studied in the literature. We discuss the ways in which these two concepts\\nare coupled. Last, we will focus on different families of machine learning approaches, how fairness\\nmanifests differently in each one, and the current state-of-the-art for tackling them in Section 5,\\nfollowed by potential areas of future work in each of the domains in Section 6.\\n2 REAL-WORLD EXAMPLES OF ALGORITHMIC UNFAIRNESS\\nWith the popularity of AI and machine learning over the past decades, and their prolific spread in\\ndifferent applications, safety and fairness constraints have become a significant issue for researchers\\nand engineers. Machine learning is used in courts to assess the probability that a defendant recommits\\na crime. It is used in different medical fields, in childhood welfare systems [ 35], and autonomous\\nvehicles. All of these applications have a direct effect in our lives and can harm our society if not\\ndesigned and engineered correctly, that is with considerations to fairness. [ 123] has a list of the\\napplications and the ways these AI systems affect our daily lives with their inherent biases, such as\\nthe existence of bias in AI chatbots, employment matching, flight routing, and automated legal aid for\\nimmigration algorithms, and search and advertising placement algorithms. [ 67] discusses examples\\nof how bias in the real world can creep into AI and robotic systems, such as bias in face recognition\\napplications, voice recognition, and search engines. Therefore, it is important for researchers and\\nengineers to be concerned about the downstream applications and their potential harmful effects\\nwhen modeling an algorithm or a system.\\n2.1 Systems that Demonstrate Discrimination\\nCOMPAS is an exemplar of a discriminatory system. In addition to this, discriminatory behavior was\\nalso evident in an algorithm that would deliver advertisements promoting jobs in Science, Technology,\\nEngineering, and Math (STEM) fields [ 88]. This advertisement was designed to deliver advertise-\\nments in a gender-neutral way. However, less women compared to men saw the advertisement due to\\ngender-imbalance which would result in younger women being considered as a valuable subgroup\\nand more expensive to show advertisements to. This optimization algorithm would deliver ads in\\na discriminatory way although its original and pure intention was to be gender-neutral. Bias in'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 1}, page_content='and more expensive to show advertisements to. This optimization algorithm would deliver ads in\\na discriminatory way although its original and pure intention was to be gender-neutral. Bias in\\n2https://www.theguardian.com/technology/2016/sep/08/artificial-intelligence-beauty-contest-doesnt-like-black-people\\n3http://content.time.com/time/business/article/0,8599,1954643,00.html'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 2}, page_content='A Survey on Bias and Fairness in Machine Learning 3\\nfacial recognition systems [ 128] and recommender systems [ 140] have also been largely studied and\\nevaluated and in many cases shown to be discriminative towards certain populations and subgroups.\\nIn order to be able to address the bias issue in these applications, it is important for us to know where\\nthese biases are coming from and what we can do to prevent them.\\nWe have enumerated the bias in COMPAS, which is a widely used commercial risk assessment\\nsoftware. In addition to its bias, it also contains performance issues when compared to humans. When\\ncompared to non-expert human judgment in a study, it was discovered to be not any better than a\\nnormal human [ 46]. It is also interesting to note that although COMPAS uses 137 features, only 7 of\\nthose were presented to the people in the study. [ 46] further argues that COMPAS is not any better\\nthan a simple logistic regression model when making decisions. We should think responsibly, and\\nrecognize that the application of these tools, and their subsequent decisions affect peoples’ lives;\\ntherefore, considering fairness constraints is a crucial task while designing and engineering these\\ntypes of sensitive tools. In another similar study, while investigating sources of group unfairness\\n(unfairness across different groups is defined later), the authors in [ 145] compared SA VRY , a tool\\nused in risk assessment frameworks that includes human intervention in its process, with automatic\\nmachine learning methods in order to see which one is more accurate and more fair. Conducting\\nthese types of studies should be done more frequently, but prior to releasing the tools in order to\\navoid doing harm.\\n2.2 Assessment Tools\\nAn interesting direction that researchers have taken is introducing tools that can assess the amount\\nof fairness in a tool or system. For example, Aequitas [ 136] is a toolkit that lets users to test\\nmodels with regards to several bias and fairness metrics for different population subgroups. Aequitas\\nproduces reports from the obtained data that helps data scientists, machine learning researchers, and\\npolicymakers to make conscious decisions and avoid harm and damage toward certain populations.\\nAI Fairness 360 (AIF360) is another toolkit developed by IBM in order to help moving fairness\\nresearch algorithms into an industrial setting and to create a benchmark for fairness algorithms to\\nget evaluated and an environment for fairness researchers to share their ideas [ 11]. These types of\\ntoolkits can be helpful for learners, researchers, and people working in the industry to move towards\\ndeveloping fair machine learning application away from discriminatory behavior.\\n3 BIAS IN DATA, ALGORITHMS, AND USER EXPERIENCES\\nMost AI systems and algorithms are data driven and require data upon which to be trained. Thus,\\ndata is tightly coupled to the functionality of these algorithms and systems. In the cases where the\\nunderlying training data contains biases, the algorithms trained on them will learn these biases and\\nreflect them into their predictions. As a result, existing biases in data can affect the algorithms using\\nthe data, producing biased outcomes. Algorithms can even amplify and perpetuate existing biases\\nin the data. In addition, algorithms themselves can display biased behavior due to certain design\\nchoices, even if the data itself is not biased. The outcomes of these biased algorithms can then be fed\\ninto real-world systems and affect users’ decisions, which will result in more biased data for training\\nfuture algorithms. For example, imagine a web search engine that puts specific results at the top of\\nits list. Users tend to interact most with the top results and pay little attention to those further down\\nthe list [ 92]. The interactions of users with items will then be collected by the web search engine,\\nand the data will be used to make future decisions on how information should be presented based'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 2}, page_content='the list [ 92]. The interactions of users with items will then be collected by the web search engine,\\nand the data will be used to make future decisions on how information should be presented based\\non popularity and user interest. As a result, results at the top will become more and more popular,\\nnot because of the nature of the result but due to the biased interaction and placement of results by\\nthese algorithms [ 92]. The loop capturing this feedback between biases in data, algorithms, and user'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 3}, page_content='4 Mehrabi et al.\\nDataAlgorithmUser\\tInteractionBehavioral\\tBiasContent\\tProduction\\tBias\\nRanking\\tBiasEmergent\\tBiasAggregation\\tBiasLongitudinal\\tData\\tFallacy\\nFig. 1. Examples of bias definitions placed in the data, algorithm, and user interaction feedback loop.\\ninteraction is illustrated in Figure 1. We use this loop to categorize definitions of bias in the section\\nbelow.\\n3.1 Types of Bias\\nBias can exist in many shapes and forms, some of which can lead to unfairness in different down-\\nstream learning tasks. In [ 144], authors talk about sources of bias in machine learning with their\\ncategorizations and descriptions in order to motivate future solutions to each of the sources of bias\\nintroduced in the paper. In [ 120], the authors prepare a complete list of different types of biases with\\ntheir corresponding definitions that exist in different cycles from data origins to its collection and its\\nprocessing. Here we will reiterate the most important sources of bias introduced in these two papers\\nand also add in some work from other existing research papers. Additionally, we will introduce a\\ndifferent categorization of these definitions in the paper according to the data, algorithm, and user\\ninteraction loop.\\n3.1.1 Data to Algorithm. In this section we talk about biases in data, which, when used by ML\\ntraining algorithms, might result in biased algorithmic outcomes.\\n(1)Measurement Bias. Measurement, or reporting, bias arises from how we choose, utilize,\\nand measure particular features [144]. An example of this type of bias was observed in the\\nrecidivism risk prediction tool COMPAS, where prior arrests and friend/family arrests were\\nused as proxy variables to measure level of “riskiness” or “crime”—-which on its own can\\nbe viewed as mismeasured proxies. This is partly due to the fact that minority communities\\nare controlled and policed more frequently, so they have higher arrest rates. However, one\\nshould not conclude that because people coming from minority groups have higher arrest rates\\ntherefore they are more dangerous as there is a difference in how these groups are assessed\\nand controlled [144].\\n(2)Omitted Variable Bias. Omitted variable bias4occurs when one or more important variables\\nare left out of the model [38,114,131]. An example for this case would be when someone'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 4}, page_content='A Survey on Bias and Fairness in Machine Learning 5\\ndesigns a model to predict, with relatively high accuracy, the annual percentage rate at which\\ncustomers will stop subscribing to a service, but soon observes that the majority of users are\\ncanceling their subscription without receiving any warning from the designed model. Now\\nimagine that the reason for canceling the subscriptions is appearance of a new strong competitor\\nin the market which offers the same solution, but for half the price. The appearance of the\\ncompetitor was something that the model was not ready for; therefore, it is considered to be an\\nomitted variable.\\n(3)Representation Bias. Representation bias arises from how we sample from a population\\nduring data collection process [144]. Non-representative samples lack the diversity of the\\npopulation, with missing subgroups and other anomalies. Lack of geographical diversity in\\ndatasets like ImageNet (as shown in Figures 3 and 4) results in demonstrable bias towards\\nWestern cultures.\\n(4)Aggregation Bias. Aggregation bias (or ecological fallacy) arises when false conclusions are\\ndrawn about individuals from observing the entire population. An example of this type of\\nbias can be seen in clinical aid tools. Consider diabetes patients who have apparent morbidity\\ndifferences across ethnicities and genders. Specifically, HbA1c levels, that are widely used\\nto diagnose and monitor diabetes, differ in complex ways across genders and ethnicities.\\nTherefore, a model that ignores individual differences will likely not be well-suited for all\\nethnic and gender groups in the population [ 144]. This is true even when they are represented\\nequally in the training data. Any general assumptions about subgroups within the population\\ncan result in aggregation bias.\\n0 50 100 150 200 250 300\\nx1050100150200250300y\\nMultivariate Linear Regression\\nClusterwise Linear Regression\\nCluster Regression\\nData\\n0 50 100 150 200 250 300\\nx1050100150200250300y\\nMultivariate Linear Regression\\nClusterwise Linear Regression\\nCluster Regression\\nData\\nFig. 2. Illustration of biases in data. The red line shows the regression (MLR) for the entire population,\\nwhile dashed green lines are regressions for each subgroup, and the solid green line is the unbiased\\nregression. (a) When all subgroups are of equal size, then MLR shows a positive relationship between\\nthe outcome and the independent variable. (b) Regression shows almost no relationship in less\\nbalanced data. The relationships between variables within each subgroup, however, remain the same.\\n(Credit: Nazanin Alipourfard)\\n(a)Simpson’s Paradox. Simpson’s paradox is a type of aggregation bias that arises in the\\nanalysis of heterogeneous data [ 18]. The paradox arises when an association observed\\nin aggregated data disappears or reverses when the same data is disaggregated into its\\nunderlying subgroups (Fig. 2(a)). One of the better-known examples of the type of paradox\\narose during the gender bias lawsuit in university admissions against UC Berkeley [ 16]. After\\nanalyzing graduate school admissions data, it seemed like there was bias toward women,\\na smaller fraction of whom were being admitted to graduate programs compared to their\\nmale counterparts. However, when admissions data was separated and analyzed over the\\ndepartments, women applicants had equality and in some cases even a small advantage'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 5}, page_content='6 Mehrabi et al.\\nFig. 3. Fraction of each country, represented by their two-letter ISO codes, in Open Images and\\nImageNet image datasets. In both datasets, US and Great Britain represent the top locations, from\\n[142]©Shreya Shankar.\\nFig. 4. Geographic distribution of countries in the Open Images data set. In their sample, almost one\\nthird of the data was US-based, and 60% of the data was from the six most represented countries\\nacross North America and Europe, from [142]©Shreya Shankar.\\nover men. The paradox happened as women tended to apply to departments with lower\\nadmission rates for both genders. Simpson’s paradox has been observed in a variety of\\ndomains, including biology [ 37], psychology [ 81], astronomy [ 109], and computational\\nsocial science [91].\\n(b)Modifiable Areal Unit Problem is a statistical bias in geospatial analysis, which arises\\nwhen modeling data at different levels of spatial aggregation [ 56]. This bias results in\\ndifferent trends learned when data is aggregated at different spatial scales.\\n(5)Sampling Bias. Sampling bias is similar to representation bias, and it arises due to non-\\nrandom sampling of subgroups. As a consequence of sampling bias, the trends estimated for\\none population may not generalize to data collected from a new population. For the intuition,\\nconsider the example in Figure 2. The left plot represents data collected during a study from\\nthree subgroups, which were uniformly sampled (Fig. 2(a)). Suppose the next time the study\\nwas conducted, one of the subgroups was sampled more frequently than the rest (Fig. 2(b)). The\\npositive trend found by the regression model in the first study almost completely disappears\\n(solid red line in plot on the right), although the subgroup trends (dashed green lines) are\\nunaffected.\\n(6)Longitudinal Data Fallacy. Researchers analyzing temporal data must use longitudinal anal-\\nysisto track cohorts over time to learn their behavior. Instead, temporal data is often modeled'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 6}, page_content='A Survey on Bias and Fairness in Machine Learning 7\\nusing cross-sectional analysis, which combines diverse cohorts at a single time point. The\\nheterogeneous cohorts can bias cross-sectional analysis, leading to different conclusions than\\nlongitudinal analysis. As an example, analysis of bulk Reddit data [ 10] revealed that comment\\nlength decreased over time on average. However, bulk data represented a cross-sectional\\nsnapshot of the population, which in reality contained different cohorts who joined Reddit\\nin different years. When data was disaggregated by cohorts, the comment length within each\\ncohort was found to increase over time.\\n(7)Linking Bias. Linking bias arises when network attributes obtained from user connections,\\nactivities, or interactions differ and misrepresent the true behavior of the users [120]. In\\n[104] authors show how social networks can be biased toward low-degree nodes when only\\nconsidering the links in the network and not considering the content and behavior of users in\\nthe network. [ 153] also shows that user interactions are significantly different from social link\\npatterns that are based on features, such as method of interaction or time. The differences and\\nbiases in the networks can be a result of many factors, such as network sampling, as shown in\\n[59, 111], which can change the network measures and cause different types of problems.\\n3.1.2 Algorithm to User. Algorithms modulate user behavior. Any biases in algorithms might\\nintroduce biases in user behavior. In this section we talk about biases that are as a result of algorithmic\\noutcomes and affect user behavior as a consequence.\\n(1)Algorithmic Bias. Algorithmic bias is when the bias is not present in the input data and is\\nadded purely by the algorithm [9]. The algorithmic design choices, such as use of certain\\noptimization functions, regularizations, choices in applying regression models on the data as\\na whole or considering subgroups, and the general use of statistically biased estimators in\\nalgorithms [ 44], can all contribute to biased algorithmic decisions that can bias the outcome of\\nthe algorithms.\\n(2)User Interaction Bias. User Interaction bias is a type of bias that can not only be observant\\non the Web but also get triggered from two sources—the user interface and through the user\\nitself by imposing his/her self-selected biased behavior and interaction [9]. This type of bias\\ncan be influenced by other types and subtypes, such as presentation and ranking biases.\\n(a)Presentation Bias. Presentation bias is a result of how information is presented [ 9].For\\nexample, on the Web users can only click on content that they see, so the seen content gets\\nclicks, while everything else gets no click. And it could be the case that the user does not see\\nall the information on the Web [9].\\n(b)Ranking Bias. The idea that top-ranked results are the most relevant and important will\\nresult in attraction of more clicks than others . This bias affects search engines [ 9] and\\ncrowdsourcing applications [93].\\n(3)Popularity Bias .Items that are more popular tend to be exposed more. However, popularity\\nmetrics are subject to manipulation—for example, by fake reviews or social bots [117]. As an\\ninstance, this type of bias can be seen in search engines [ 71,117] or recommendation systems\\nwhere popular objects would be presented more to the public. But this presentation may not be\\na result of good quality; instead, it may be due to other biased factors.\\n(4)Emergent Bias. Emergent bias occurs as a result of use and interaction with real users. This\\nbias arises as a result of change in population, cultural values, or societal knowledge usually\\nsome time after the completion of design [53]. This type of bias is more likely to be observed\\nin user interfaces, since interfaces tend to reflect the capacities, characteristics, and habits of\\nprospective users by design [ 53]. This type of bias can itself be divided into more subtypes, as\\ndiscussed in detail in [53].'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 7}, page_content='8 Mehrabi et al.\\n(5)Evaluation Bias. Evaluation bias happens during model evaluation [144]. This includes the\\nuse of inappropriate and disproportionate benchmarks for evaluation of applications such\\nas Adience and IJB-A benchmarks. These benchmarks are used in the evaluation of facial\\nrecognition systems that were biased toward skin color and gender [ 24], and can serve as\\nexamples for this type of bias [144].\\n3.1.3 User to Data. Many data sources used for training ML models are user-generated. Any\\ninherent biases in users might be reflected in the data they generate. Furthermore, when user behavior\\nis affected/modulated by an algorithm, any biases present in those algorithm might introduce bias in\\nthe data generation process. Here we list several important types of such biases.\\n(1)Historical Bias. Historical bias is the already existing bias and socio-technical issues in the\\nworld and can seep into from the data generation process even given a perfect sampling and\\nfeature selection [144]. An example of this type of bias can be found in a 2018 image search\\nresult where searching for women CEOs ultimately resulted in fewer female CEO images due\\nto the fact that only 5% of Fortune 500 CEOs were woman—which would cause the search\\nresults to be biased towards male CEOs [ 144]. These search results were of course reflecting\\nthe reality, but whether or not the search algorithms should reflect this reality is an issue worth\\nconsidering.\\n(2)Population Bias. Population bias arises when statistics, demographics, representatives, and\\nuser characteristics are different in the user population of the platform from the original target\\npopulation [120]. Population bias creates non-representative data. An example of this type of\\nbias can arise from different user demographics on different social platforms, such as women\\nbeing more likely to use Pinterest, Facebook, Instagram, while men being more active in online\\nforums like Reddit or Twitter. More such examples and statistics related to social media use\\namong young adults according to gender, race, ethnicity, and parental educational background\\ncan be found in [64].\\n(3)Self-Selection Bias. Self-selection bias4is a subtype of the selection or sampling bias in which\\nsubjects of the research select themselves. An example of this type of bias can be observed in\\nan opinion poll to measure enthusiasm for a political candidate, where the most enthusiastic\\nsupporters are more likely to complete the poll.\\n(4)Social Bias. Social bias happens when others’ actions affect our judgment. [9]. An example\\nof this type of bias can be a case where we want to rate or review an item with a low score, but\\nwhen influenced by other high ratings, we change our scoring thinking that perhaps we are\\nbeing too harsh [9, 151].\\n(5)Behavioral Bias. Behavioral bias arises from different user behavior across platforms, con-\\ntexts, or different datasets [120]. An example of this type of bias can be observed in [ 108],\\nwhere authors show how differences in emoji representations among platforms can result in\\ndifferent reactions and behavior from people and sometimes even leading to communication\\nerrors.\\n(6)Temporal Bias. Temporal bias arises from differences in populations and behaviors over time\\n[120]. An example can be observed in Twitter where people talking about a particular topic\\nstart using a hashtag at some point to capture attention, then continue the discussion about the\\nevent without using the hashtag [120, 146].\\n(7)Content Production Bias. Content Production bias arises from structural, lexical, semantic,\\nand syntactic differences in the contents generated by users [120]. An example of this type of\\nbias can be seen in [ 118] where the differences in use of language across different gender and\\n4https://data36.com/statistical-bias-types-explained/'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 8}, page_content='A Survey on Bias and Fairness in Machine Learning 9\\nage groups is discussed. The differences in use of language can also be seen across and within\\ncountries and populations.\\nExisting work tries to categorize these bias definitions into groups, such as definitions falling solely\\nunder data or user interaction. However, due to the existence of the feedback loop phenomenon [ 36],\\nthese definitions are intertwined, and we need a categorization which closely models this situation.\\nThis feedback loop is not only existent between the data and the algorithm, but also between the\\nalgorithms and user interaction [ 29]. Inspired by these papers, we modeled categorization of bias\\ndefinitions, as shown in Figure 1, and grouped these definitions on the arrows of the loop where we\\nthought they were most effective. We emphasize the fact again that these definitions are intertwined,\\nand one should consider how they affect each other in this cycle, and address them accordingly.\\n3.2 Data Bias Examples\\nThere are multiple ways that discriminatory bias can seep into data. For instance, using unbalanced\\ndata can create biases against underrepresented groups. [ 170] analyzes some examples of the biases\\nthat can exist in the data and algorithms and offer some recommendations and suggestions toward\\nmitigating these issues.\\n3.2.1 Examples of Bias in Machine Learning Data. In [24], the authors show that datasets like\\nIJB-A and Adience are imbalanced and contain mainly light-skinned subjects—79.6% in IJB-A and\\n86.2% in Adience. This can bias the analysis towards dark-skinned groups who are underrepresented\\nin the data. In another instance, the way we use and analyze our data can create bias when we do not\\nconsider different subgroups in the data. In [ 24], the authors also show that considering only male-\\nfemale groups is not enough, but there is also a need to use race to further subdivide the gender groups\\ninto light-skinned females, light-skinned males, dark-skinned males, and dark-skinned females. It’s\\nonly in this case that we can clearly observe the bias towards dark-skinned females, as previously\\ndark-skinned males would compromise for dark-skinned females and would hide the underlying\\nbias towards this subgroup. Popular machine-learning datasets that serve as a base for most of\\nthe developed algorithms and tools can also be biased—which can be harmful to the downstream\\napplications that are based on these datasets. For instance, ImageNet [ 135] and Open Images [ 86] are\\ntwo widely used datasets in machine-learning. In [ 142], researchers showed that these datasets suffer\\nfrom representation bias and advocate for the need to incorporate geographic diversity and inclusion\\nwhile creating such datasets. In addition, authors in [ 105] write about the existing representational\\nbiases in different knowledge bases that are widely used in Natural Language Processing (NLP)\\napplications for different commonsense reasoning tasks.\\n3.2.2 Examples of Data Bias in Medical Applications. These data biases can be more dangerous\\nin other sensitive applications. For example, in medical domains there are many instances in which\\nthe data studied and used are skewed toward certain populations—which can have dangerous conse-\\nquences for the underrepresented communities. [ 98] showed how exclusion of African-Americans\\nresulted in their misclassification in clinical studies, so they became advocates for sequencing the\\ngenomes of diverse populations in the data to prevent harm to underrepresented populations. Authors\\nin [143] studied the 23andMe genotype dataset and found that out of 2,399 individuals, who have\\nopenly shared their genotypes in public repositories, 2,098 (87%) are European, while only 58\\n(2%) are Asian and 50 (2%) African. Other such studies were conducted in [ 54] which states that\\nUK Biobank, a large and widely used genetic dataset, may not represent the sampling population.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 8}, page_content='(2%) are Asian and 50 (2%) African. Other such studies were conducted in [ 54] which states that\\nUK Biobank, a large and widely used genetic dataset, may not represent the sampling population.\\nResearchers found evidence of a “healthy volunteer” selection bias. [ 150] has other examples of\\nstudies on existing biases in the data used in the medical domain. [ 157] also looks at machine-learning'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 9}, page_content='10 Mehrabi et al.\\nalgorithms and data utilized in medical fields, and writes about how artificial intelligence in health\\ncare has not impacted all patients equally.\\n3.3 Discrimination\\nSimilar to bias, discrimination is also a source of unfairness. Discrimination can be considered as a\\nsource for unfairness that is due to human prejudice and stereotyping based on the sensitive attributes,\\nwhich may happen intentionally or unintentionally, while bias can be considered as a source for\\nunfairness that is due to the data collection, sampling, and measurement. Although bias can also be\\nseen as a source of unfairness that is due to human prejudice and stereotyping, in the algorithmic\\nfairness literature it is more intuitive to categorize them as such according to the existing research\\nin these areas. In this survey, we mainly focus on concepts that are relevant to algorithmic fairness\\nissues. [ 99,133,152] contain more broad information on discrimination theory that involve more\\nmultidisciplinary concepts from legal theory, economics, and social sciences which can be referenced\\nby the interested readers.\\n3.3.1 Explainable Discrimination. Differences in treatment and outcomes amongst different\\ngroups can be justified and explained via some attributes in some cases. In situations where these\\ndifferences are justified and explained, it is not considered to be illegal discrimination and hence\\ncalled explainable [ 77]. For instance, authors in [ 77] state that in the UCI Adult dataset [ 7], a widely\\nused dataset in the fairness domain, males on average have a higher annual income than females.\\nHowever, this is because on average females work fewer hours than males per week. Work hours\\nper week is an attribute that can be used to explain low income which needs to be considered. If we\\nmake decisions, without considering working hours, such that males and females end up averaging\\nthe same income, we will lead to reverse discrimination since we would cause male employees to get\\nlower salary than females. Therefore, explainable discrimination is acceptable and legal as it can\\nbe explained through other attributes like working hours. In [ 77], authors present a methodology\\nto quantify the explainable and illegal discrimination in data. They argue that methods that do not\\ntake the explainable part of the discrimination into account may result in non-desirable outcomes, so\\nthey introduce a reverse discrimination which is equally harmful and undesirable. They explain how\\nto quantify and measure discrimination in data or a classifier’s decisions which directly considers\\nillegal and explainable discrimination.\\n3.3.2 Unexplainable Discrimination. In contrast to explainable discrimination, there is unexplain-\\nable discrimination in which the discrimination toward a group is unjustified and therefore considered\\nillegal. Authors in [ 77] also present local techniques for removing only the illegal or unexplainable\\ndiscrimination, allowing only for explainable differences in decisions. These are preprocessing tech-\\nniques that change the training data such that it contains no unexplainable discrimination. We expect\\nclassifiers trained on this preprocessed data to not capture illegal or unexplainable discrimination.\\nUnexplainable discrimination consists of direct andindirect discrimination.\\n(1)Direct Discrimination. Direct discrimination happens when protected attributes of individuals\\nexplicitly result in non-favorable outcomes toward them [ 164]. Typically, there are some traits\\nidentified by law on which it is illegal to discriminate against, and it is usually these traits that\\nare considered to be “protected” or “sensitive” attributes in computer science literature. A list\\nof some of these protected attributes is provided in Table 3 as specified in the Fair Housing\\nand Equal Credit Opportunity Acts (FHA and ECOA) [30].\\n(2)Indirect Discrimination. In indirect discrimination, individuals appear to be treated based'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 9}, page_content='and Equal Credit Opportunity Acts (FHA and ECOA) [30].\\n(2)Indirect Discrimination. In indirect discrimination, individuals appear to be treated based\\non seemingly neutral and non-protected attributes; however, protected groups, or individuals\\nstill get to be treated unjustly as a result of implicit effects from their protected attributes (e.g.,'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 10}, page_content='A Survey on Bias and Fairness in Machine Learning 11\\nthe residential zip code of a person can be used in decision making processes such as loan\\napplications. However, this can still lead to racial discrimination, such as redlining, as despite\\nthe fact that zip code appears to be a non-sensitive attribute, it may correlate with race because\\nof the population of residential areas.) [130, 164].\\n3.3.3 Sources of Discrimination.\\n(1)Systemic Discrimination. Systemic discrimination refers to policies, customs, or behaviors\\nthat are a part of the culture or structure of an organization that may perpetuate discrimination\\nagainst certain subgroups of the population [ 40]. [132] found that employers overwhelmingly\\npreferred competent candidates that were culturally similar to them, and shared similar ex-\\nperiences and hobbies. If the decision-makers happen to belong overwhelmingly to certain\\nsubgroups, this may result in discrimination against competent candidates that do not belong\\nto these subgroups.\\n(2)Statistical Discrimination. Statistical discrimination is a phenomenon where decision-makers\\nuse average group statistics to judge an individual belonging to that group. It usually occurs\\nwhen the decision-makers (e.g., employers, or law enforcement officers) use an individual’s\\nobvious, recognizable characteristics as a proxy for either hidden or more-difficult-to-determine\\ncharacteristics, that may actually be relevant to the outcome [124].\\n4 ALGORITHMIC FAIRNESS\\nFighting against bias and discrimination has a long history in philosophy and psychology, and\\nrecently in machine-learning. However, in order to be able to fight against discrimination and achieve\\nfairness, one should first define fairness. Philosophy and psychology have tried to define the concept\\nof fairness long before computer science. The fact that no universal definition of fairness exists shows\\nthe difficulty of solving this problem [138]. Different preferences and outlooks in different cultures\\nlend a preference to different ways of looking at fairness, which makes it harder to come up with just\\na single definition that is acceptable to everyone in a situation. Indeed, even in computer science,\\nwhere most of the work on proposing new fairness constraints for algorithms has come from the\\nWest, and a lot of these papers use the same datasets and problems to show how their constraints\\nperform, there is still no clear agreement on which constraints are the most appropriate for those\\nproblems. Broadly, fairness is the absence of any prejudice or favoritism towards an individual or a\\ngroup based on their intrinsic or acquired traits in the context of decision-making [ 139]. Even though\\nfairness is an incredibly desirable quality in society, it can be surprisingly difficult to achieve in\\npractice. With these challenges in mind, many fairness definitions are proposed to address different\\nalgorithmic bias and discrimination issues discussed in the previous section.\\n4.1 Definitions of Fairness\\nIn [17], authors studied fairness definitions in political philosophy and tried to tie them to machine-\\nlearning. Authors in [ 70] studied the 50-year history of fairness definitions in the areas of education\\nand machine-learning. In [ 149], authors listed and explained some of the definitions used for fairness\\nin algorithmic classification problems. In [ 139], authors studied the general public’s perception of\\nsome of these fairness definitions in computer science literature. Here we will reiterate and provide\\nsome of the most widely used definitions, along with their explanations inspired from [149].\\nDefinition 1. (Equalized Odds). The definition of equalized odds, provided by [ 63], states that\\n“A predictor ˆY satisfies equalized odds with respect to protected attribute A and outcome Y, if ˆY and A\\nare independent conditional on Y. P( ˆY=1|A=0,Y =y) = P( ˆY=1|A=1,Y =y) , y∈{0,1}” . This means'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 10}, page_content='are independent conditional on Y. P( ˆY=1|A=0,Y =y) = P( ˆY=1|A=1,Y =y) , y∈{0,1}” . This means\\nthat the probability of a person in the positive class being correctly assigned a positive outcome'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 11}, page_content='12 Mehrabi et al.\\nand the probability of a person in a negative class being incorrectly assigned a positive outcome\\nshould both be the same for the protected and unprotected group members [ 149]. In other words, the\\nequalized odds definition states that the protected and unprotected groups should have equal rates for\\ntrue positives and false positives.\\nDefinition 2. (Equal Opportunity). “A binary predictor ˆY satisfies equal opportunity with respect\\nto A and Y if P( ˆY=1|A=0,Y=1) = P( ˆY=1|A=1,Y=1)” [63]. This means that the probability of a\\nperson in a positive class being assigned to a positive outcome should be equal for both protected\\nand unprotected (female and male) group members [ 149]. In other words, the equal opportunity\\ndefinition states that the protected and unprotected groups should have equal true positive rates.\\nDefinition 3. (Demographic Parity). Also known as statistical parity. “A predictor ˆY satisfies demo-\\ngraphic parity if P( ˆY |A = 0) = P( ˆY|A = 1)” [48,87]. The likelihood of a positive outcome [ 149]\\nshould be the same regardless of whether the person is in the protected (e.g., female) group.\\nDefinition 4. (Fairness Through Awareness). “An algorithm is fair if it gives similar predictions to\\nsimilar individuals” [48,87]. In other words, any two individuals who are similar with respect to a\\nsimilarity (inverse distance) metric defined for a particular task should receive a similar outcome.\\nDefinition 5. (Fairness Through Unawareness). “An algorithm is fair as long as any protected\\nattributes A are not explicitly used in the decision-making process” [61, 87].\\nDefinition 6. (Treatment Equality). “Treatment equality is achieved when the ratio of false negatives\\nand false positives is the same for both protected group categories” [15].\\nDefinition 7. (Test Fairness). “A score S = S(x) is test fair (well-calibrated) if it reflects the same\\nlikelihood of recidivism irrespective of the individual’s group membership, R. That is, if for all values\\nof s, P(Y =1|S=s,R=b)=P(Y =1|S=s,R=w)” [34]. In other words, the test fairness definition states\\nthat for any predicted probability score S, people in both protected and unprotected groups must have\\nequal probability of correctly belonging to the positive class [149].\\nDefinition 8. (Counterfactual Fairness). “Predictor ˆY is counterfactually fair if under any con-\\ntext X =x and A=a, P( ˆ𝑌𝐴← −𝑎(U)=y|X =x,A=a)=P( ˆ𝑌𝐴← −𝑎′(U)=y|X =x,A=a), (for all y and for any\\nvalue𝑎′attainable by A” [87]. The counterfactual fairness definition is based on the “intuition that a\\ndecision is fair towards an individual if it is the same in both the actual world and a counterfactual\\nworld where the individual belonged to a different demographic group.”\\nDefinition 9. (Fairness in Relational Domains). “A notion of fairness that is able to capture the\\nrelational structure in a domain—not only by taking attributes of individuals into consideration but\\nby taking into account the social, organizational, and other connections between individuals” [50].\\nDefinition 10. (Conditional Statistical Parity). For a set of legitimate factors L, predictor ˆY satisfies\\nconditional statistical parity if P( ˆY |L=1,A = 0) = P( ˆY|L=1,A = 1) [41]. Conditional statistical parity\\nstates that people in both protected and unprotected (female and male) groups should have equal\\nprobability of being assigned to a positive outcome given a set of legitimate factors L [149].\\nFairness definitions fall under different types as follows:'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 12}, page_content='A Survey on Bias and Fairness in Machine Learning 13\\nName Reference Group Subgroup Individual\\nDemographic parity [87][48] ✓\\nConditional statistical parity [41] ✓\\nEqualized odds [63] ✓\\nEqual opportunity [63] ✓\\nTreatment equality [15] ✓\\nTest fairness [34] ✓\\nSubgroup fairness [79][80] ✓\\nFairness through unawareness [87][61] ✓\\nFairness through awareness [48] ✓\\nCounterfactual fairness [87] ✓\\nTable 1. Categorizing different fairness notions into group, subgroup, and individual types.\\n(1)Individual Fairness. Give similar predictions to similar individuals [48, 87].\\n(2)Group Fairness. Treat different groups equally [48, 87].\\n(3)Subgroup Fairness . Subgroup fairness intends to obtain the best properties of the group and\\nindividual notions of fairness. It is different than these notions but uses them in order to obtain\\nbetter outcomes. It picks a group fairness constraint like equalizing false positive and asks\\nwhether this constraint holds over a large collection of subgroups [79, 80].\\nIt is important to note that according to [ 83], it is impossible to satisfy some of the fairness con-\\nstraints at once except in highly constrained special cases. In [ 83], the authors show the inherent\\nincompatibility of two conditions: calibration and balancing the positive and negative classes. These\\ncannot be satisfied simultaneously with each other unless under certain constraints; therefore, it\\nis important to take the context and application in which fairness definitions need to be used into\\nconsideration and use them accordingly [ 141]. Another important aspect to consider is time and\\ntemporal analysis of the impacts that these definitions may have on individuals or groups. In [ 95]\\nauthors show that current fairness definitions are not always helpful and do not promote improvement\\nfor sensitive groups—and can actually be harmful when analyzed over time in some cases. They\\nalso show that measurement errors can also act in favor of these fairness definitions; therefore, they\\nshow how temporal modeling and measurement are important in evaluation of fairness criteria and\\nintroduce a new range of trade-offs and challenges toward this direction. It is also important to pay\\nattention to the sources of bias and their types when trying to solve fairness-related questions.\\n5 METHODS FOR FAIR MACHINE LEARNING\\nThere have been numerous attempts to address bias in artificial intelligence in order to achieve\\nfairness; these stem from domains of AI. In this section we will enumerate different domains of\\nAI, and the work that has been produced by each community to combat bias and unfairness in their\\nmethods. Table 2 provides an overview of the different areas that we focus upon in this survey.\\nWhile this section is largely domain-specific, it can be useful to take a cross-domain view. Gener-\\nally, methods that target biases in the algorithms fall under three categories:\\n(1)Pre-processing. Pre-processing techniques try to transform the data so that the underlying\\ndiscrimination is removed [ 43]. If the algorithm is allowed to modify the training data, then\\npre-processing can be used [11].\\n(2)In-processing. In-processing techniques try to modify and change state-of-the-art learning\\nalgorithms in order to remove discrimination during the model training process [ 43]. If it is'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 13}, page_content='14 Mehrabi et al.\\nallowed to change the learning procedure for a machine learning model, then in-processing\\ncan be used during the training of a model— either by incorporating changes into the objective\\nfunction or imposing a constraint [11, 14].\\n(3)Post-processing. Post-processing is performed after training by accessing a holdout set which\\nwas not involved during the training of the model [ 43]. If the algorithm can only treat the\\nlearned model as a black box without any ability to modify the training data or learning\\nalgorithm, then only post-processing can be used in which the labels assigned by the black-box\\nmodel initially get reassigned based on a function during the post-processing phase [11, 14].\\nExamples of some existing work and their categorization into these types is shown in Table 4. These\\nmethods are not just limited to general machine learning techniques, but because of AI’s popularity,\\nthey have expanded to different domains such as natural language processing and deep learning. From\\nlearning fair representations [ 42,97,112] to learning fair word embeddings [ 20,58,169], debiasing\\nmethods have been proposed in different AI applications and domains. Most of these methods try\\nto avoid unethical interference of sensitive or protected attributes into the decision-making process,\\nwhile others target exclusion bias by trying to include users from sensitive groups. In addition, some\\nworks try to satisfy one or more of the fairness notions in their methods, such as disparate learning\\nprocesses (DLPs) which try to satisfy notions of treatment disparity and impact disparity by allowing\\nthe protected attributes during the training phase but avoiding them during prediction time [ 94]. A\\nlist of protected or sensitive attributes is provided in Table 3. They point out what attributes should\\nnot affect the outcome of the decision in housing loan or credit card decision-making [ 30] according\\nto the law. Some of the existing work tries to treat sensitive attributes as noise to disregard their\\neffect on decision-making, while some causal methods use causal graphs, and disregard some paths\\nin the causal graph that result in sensitive attributes affecting the outcome of the decision. Different\\nbias-mitigating methods and techniques are discussed below for different domains—each targeting a\\ndifferent problem in different areas of machine learning in detail. This can expand the horizon of\\nthe reader on where and how bias can affect the system and try to help researchers carefully look\\nat various new problems concerning potential places where discrimination and bias can affect the\\noutcome of a system.\\n5.1 Unbiasing Data\\nEvery dataset is the result of several design decisions made by the data curator. Those decisions have\\nconsequences for the fairness of the resulting dataset, which in turn affects the resulting algorithms. In\\norder to mitigate the effects of bias in data, some general methods have been proposed that advocate\\nhaving good practices while using data, such as having datasheets that would act like a supporting\\ndocument for the data reporting the dataset creation method, its characteristics, motivations, and its\\nskews [ 13,55]. [12] proposes a similar approach for the NLP applications. A similar suggestion has\\nbeen proposed for models in [ 110]. Authors in [ 66] also propose having labels, just like nutrition\\nlabels on food, in order to better categorize each data for each task. In addition to these general\\ntechniques, some work has targeted more specific types of biases. For example, [ 81] has proposed\\nmethods to test for cases of Simpson’s paradox in the data, and [ 3,4] proposed methods to discover\\nSimpson’s paradoxes in data automatically. Causal models and graphs were also used in some work\\nto detect direct discrimination in the data along with its prevention technique that modifies the data\\nsuch that the predictions would be absent from direct discrimination [ 163]. [62] also worked on'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 13}, page_content='to detect direct discrimination in the data along with its prevention technique that modifies the data\\nsuch that the predictions would be absent from direct discrimination [ 163]. [62] also worked on\\npreventing discrimination in data mining, targeting direct, indirect, and simultaneous effects. Other\\npre-processing approaches, such as messaging [ 74], preferential sampling [ 75,76], disparate impact\\nremoval [51], also aim to remove biases from the data.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 14}, page_content='A Survey on Bias and Fairness in Machine Learning 15\\nArea Reference(s)\\nClassification [78] [106] [57] [85] [147] [63] [159] [154] [69]\\n[25] [155] [122] [49] [73] [75]\\nRegression [14] [1]\\nPCA [137]\\nCommunity detection [104]\\nClustering [31] [8]\\nGraph embedding [22]\\nCausal inference [96] [164] [165] [160] [116] [115] [162] [82] [127]\\n[161]\\nVariational auto encoders [97] [5] [112] [42]\\nAdversarial learning [90] [156]\\nWord embedding [20] [169] [58] [23] [166]\\nCoreference resolution [168] [134]\\nLanguage model [21]\\nSentence embedding [100]\\nMachine translation [52]\\nSemantic role labeling [167]\\nNamed Entity Recognition [101]\\nTable 2. List of papers targeting and talking about bias and fairness in different areas.\\nAttribute FHA ECOA\\nRace ✓ ✓\\nColor ✓ ✓\\nNational origin ✓ ✓\\nReligion ✓ ✓\\nSex ✓ ✓\\nFamilial status ✓\\nDisability ✓\\nExercised rights under CCPA ✓\\nMarital status ✓\\nRecipient of public assistance ✓\\nAge ✓\\nTable 3. A list of the protected attributes as specified in the Fair Housing and Equal Credit Opportunity\\nActs (FHA and ECOA), from [30].\\n5.2 Fair Machine Learning\\nTo address this issue, a variety of methods have been proposed that satisfy some of the fairness\\ndefinitions or other new definitions depending on the application.\\n5.2.1 Fair Classification. Since classification is a canonical task in machine learning and is\\nwidely used in different areas that can be in direct contact with humans, it is important that these\\ntypes of methods be fair and be absent from biases that can harm some populations. Therefore,\\ncertain methods have been proposed [ 57,78,85,106] that satisfy certain definitions of fairness in\\nclassification. For instance, in [ 147] authors try to satisfy subgroup fairness in classification, equality\\nof opportunity and equalized odds in [ 63], both disparate treatment and disparate impact in [ 2,159],'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 15}, page_content='16 Mehrabi et al.\\nand equalized odds in [ 154]. Other methods try to not only satisfy some fairness constraints but to\\nalso be stable toward change in the test set [ 69]. The authors in [ 155], propose a general framework\\nfor learning fair classifiers. This framework can be used for formulating fairness-aware classification\\nwith fairness guarantees. In another work [ 25], authors propose three different modifications to the\\nexisting Naive Bayes classifier for discrimination-free classification. [ 122] takes a new approach\\ninto fair classification by imposing fairness constraints into a Multitask learning (MTL) framework.\\nIn addition to imposing fairness during training, this approach can benefit the minority groups by\\nfocusing on maximizing the average accuracy of each group as opposed to maximizing the accuracy\\nas a whole without attention to accuracy across different groups. In a similar work [ 49], authors\\npropose a decoupled classification system where a separate classifier is learned for each group. They\\nuse transfer learning to reduce the issue of having less data for minority groups. In [ 73] authors\\npropose to achieve fair classification by mitigating the dependence of the classification outcome on\\nthe sensitive attributes by utilizing the Wasserstein distance measure. In [ 75] authors propose the\\nPreferential Sampling (PS) method to create a discrimination free train data set. They then learn\\na classifier on this discrimination free dataset to have a classifier with no discrimination. In [ 102],\\nauthors propose a post-processing bias mitigation strategy that utilizes attention mechanism for\\nclassification and that can provide interpretability.\\nAlgorithm Reference Pre-Processing In-Processing Post-Processing\\nCommunity detection [104] ✓\\nWord embedding [23] ✓\\nOptimized pre-processing [27] ✓\\nData pre-processing [76] ✓\\nClassification [159] ✓\\nRegression [14] ✓\\nClassification [78] ✓\\nClassification [155] ✓\\nAdversarial learning [90] ✓\\nClassification [63] ✓\\nWord embedding [20] ✓\\nClassification [125] ✓\\nClassification [102] ✓\\nTable 4. Algorithms categorized into their appropriate groups based on being pre-processing, in-\\nprocessing, or post-processing.\\n5.2.2 Fair Regression. [14] proposes a fair regression method along with evaluating it with a\\nmeasure introduced as the “price of fairness” (POF) to measure accuracy-fairness trade-offs. They\\nintroduce three fairness penalties as follows:\\nIndividual Fairness: The definition for individual fairness as stated in [ 14], “for every cross pair\\n(𝑥,𝑦)∈𝑆1,(𝑥′,𝑦′)∈𝑆2, a model𝑤is penalized for how differently it treats 𝑥and𝑥′(weighted by a\\nfunction of|𝑦−𝑦′|) where𝑆1and𝑆2are different groups from the sampled population.” Formally,\\nthis is operationalized as\\n𝑓1(𝑤,𝑆)=1\\n𝑛1𝑛2∑︁\\n(𝑥𝑖,𝑦𝑖)∈𝑆1\\n(𝑥𝑗,𝑦𝑗)∈𝑆2𝑑(𝑦𝑖,𝑦𝑗)(𝑤.𝑥𝑖−𝑤.𝑥𝑗)2'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 16}, page_content='A Survey on Bias and Fairness in Machine Learning 17\\nGroup Fairness: \"On average, the two groups’ instances should have similar labels (weighted by the\\nnearness of the labels of the instances)\" [14].\\n𝑓2(𝑤,𝑆)= \\n1\\n𝑛1𝑛2∑︁\\n(𝑥𝑖,𝑦𝑖)∈𝑆1\\n(𝑥𝑗,𝑦𝑗)∈𝑆2𝑑(𝑦𝑖,𝑦𝑗)(𝑤.𝑥𝑖−𝑤.𝑥𝑗)!2\\nHybrid Fairness: \"Hybrid fairness requires both positive and both negatively labeled cross pairs to be\\ntreated similarly in an average over the two groups\" [14].\\n𝑓3(𝑤,𝑆)= ∑︁\\n(𝑥𝑖,𝑦𝑖)∈𝑆1\\n(𝑥𝑗,𝑦𝑗)∈𝑆2\\n𝑦𝑖=𝑦𝑗=1𝑑(𝑦𝑖,𝑦𝑗)(𝑤.𝑥𝑖−𝑤.𝑥𝑗)\\n𝑛1,1𝑛2,1!2\\n+ ∑︁\\n(𝑥𝑖,𝑦𝑖)∈𝑆1\\n(𝑥𝑗,𝑦𝑗)∈𝑆2\\n𝑦𝑖=𝑦𝑗=−1𝑑(𝑦𝑖,𝑦𝑗)(𝑤.𝑥𝑖−𝑤.𝑥𝑗)\\n𝑛1,−1𝑛2,−1!2\\nIn addition to the previous work, [ 1] considers the fair regression problem formulation with regards\\nto two notions of fairness statistical (demographic) parity and bounded group loss. [ 2] uses decision\\ntrees to satisfy disparate impact and treatment in regression tasks in addition to classification.\\n5.2.3 Structured Prediction. In [167], authors studied the semantic role-labeling models and a\\nfamous dataset, imSitu, and realized that only 33% of agent roles in cooking images are man, and\\nthe rest of 67% cooking images have woman as agents in the imSitu training set. They also noticed\\nthat in addition to the existing bias in the dataset, the model would amplify the bias such that after\\ntraining a model5on the dataset, bias is magnified for “man”, filling only 16% of cooking images.\\nUnder these observations, the authors of the paper [ 167] show that structured prediction models\\nhave the risk of leveraging social bias. Therefore, they propose a calibration algorithm called RBA\\n(reducing bias amplification); RBA is a technique for debiasing models by calibrating prediction in\\nstructured prediction. The idea behind RBA is to ensure that the model predictions follow the same\\ndistribution in the training data. They study two cases: multi-label object and visual semantic role\\nlabeling classification. They show how these methods amplify the existing bias in data.\\n5.2.4 Fair PCA. In [137] authors show that vanilla PCA can exaggerate the error in reconstruction\\nin one group of people over a different group of equal size, so they propose a fair method to create\\nrepresentations with similar richness for different populations—not to make them indistinguishable,\\nor to hide dependence on a sensitive or protected attribute. They show that vanilla PCA on the\\nlabeled faces in the wild (LFW) dataset [ 68] has a lower reconstruction error rate for men than for\\nwomen faces, even if the sampling is done with an equal weight for both genders. They intend to\\nintroduce a dimensionality reduction technique which maintains similar fidelity for different groups\\nand populations in the dataset. Therefore, they introduce Fair PCA and define a fair dimensionality\\nreduction algorithm. Their definition of Fair PCA (as an optimization function) is as follows, in\\nwhich𝐴and𝐵denote two subgroups, 𝑈𝐴and𝑈𝐵denote matrices whose rows correspond to rows of\\n𝑈that contain members of subgroups 𝐴and𝐵given𝑚data points in 𝑅𝑛:\\n𝑚𝑖𝑛𝑈∈𝑅𝑚×𝑛,𝑟𝑎𝑛𝑘(𝑈)≤𝑑𝑚𝑎𝑥(\\n1\\n|𝐴|𝑙𝑜𝑠𝑠(𝐴,𝑈𝐴),1\\n|𝐵|𝑙𝑜𝑠𝑠(𝐵,𝑈𝐵))\\nAnd their proposed algorithm is a two-step process listed below:\\n(1) Relax the Fair PCA objective to a semidefinite program (SDP) and solve it.\\n(2) Solve a linear program that would reduce the rank of the solution.\\n5Specifically, a Conditional Random Field (CRF)'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 17}, page_content='18 Mehrabi et al.\\n5.2.5 Community Detection/Graph Embedding/Clustering. Inequalities in online communities\\nand social networks can also potentially be another place where bias and discrimination can affect the\\npopulations. For example, in online communities users with a fewer number of friends or followers\\nface a disadvantage of being heard in online social media [ 104]. In addition, existing methods, such\\nas community detection methods, can amplify this bias by ignoring these low-connected users in\\nthe network or by wrongfully assigning them to the irrelevant and small communities. In [ 104]\\nauthors show how this type of bias exists and is perpetuated by the existing community detection\\nmethods. They propose a new attributed community detection method, called CLAN, to mitigate\\nthe harm toward disadvantaged groups in online social communities. CLAN is a two-step process\\nthat considers the network structure alongside node attributes to address exclusion bias, as indicated\\nbelow:\\n(1)Detect communities using modularity values (Step 1-unsupervised using only network struc-\\nture).\\n(2)Train a classifier to classify users in the minor groups, putting them into one of the major\\ngroups using held-out node attributes (Step 2-supervised using other node attributes).\\nFair methods in domains similar to community detection are also proposed, such as graph embedding\\n[22] and clustering [8, 31].\\n5.2.6 Causal Approach to Fairness. Causal models can ascertain causal relationships between\\nvariables. Using causal graphs one can represent these causal relationships between variables (nodes\\nof the graph) through the edges of the graph. These models can be used to remove unwanted causal\\ndependence of outcomes on sensitive attributes such as gender or race in designing systems or policies\\n[96]. Many researchers have used causal models and graphs to solve fairness-related concerns in\\nmachine learning. In [ 33,96], authors discuss in detail the subject of causality and its importance\\nwhile designing fair algorithms. There has been much research on discrimination discovery and re-\\nmoval that uses causal models and graphs in order to make decisions that are irrespective of sensitive\\nattributes of groups or individuals. For instance, in [ 164] authors propose a causal-based framework\\nthat detects direct and indirect discrimination in the data along with their removal techniques. [ 165]\\nis an extension to the previous work. [ 160] gives a nice overview of most of the previous work done\\nin this area by the authors, along with discussing system-, group-, and individual-level discrimination\\nand solving each using their previous methods, in addition to targeting direct and indirect discrimi-\\nnation. By expanding on the previous work and generalizing it, authors in [ 116] propose a similar\\npathway approach for fair inference using causal graphs; this would restrict certain problematic and\\ndiscriminative pathways in the causal graph flexibly given any set of constraints. This holds when\\nthe path-specific effects can be identified from the observed distribution. In [ 32] authors introduce\\nthe path-specific counterfactual fairness definition which is an extension to counterfactual fairness\\ndefinition [ 87] and propose a method to achieve it further extending the work in [ 116]. In [ 115]\\nauthors extended a formalization of algorithmic fairness from their previous work to the setting of\\nlearning optimal policies that are subject to constraints based on definitions of fairness. They describe\\nseveral strategies for learning optimal policies by modifying some of the existing strategies, such as\\nQ-learning, value search, and G-estimation, based on some fairness considerations. In [ 162] authors\\nonly target discrimination discovery and no removal by finding instances similar to another instance\\nand observing if a change in the protected attribute will change the outcome of the decision. If so,'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 17}, page_content='and observing if a change in the protected attribute will change the outcome of the decision. If so,\\nthey declare the existence of discrimination. In [ 82], authors define the following two notions of\\ndiscrimination—unresolved discrimination and proxy discrimination—as follows:\\nUnresolved Discrimination: \"A variable V in a causal graph exhibits unresolved discrimination if\\nthere exists a directed path from A to V that is not blocked by a resolving variable, and V itself is\\nnon-resolving\" [82].'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 18}, page_content='A Survey on Bias and Fairness in Machine Learning 19\\nProxy Discrimination: \"A variable V in a causal graph exhibits potential proxy discrimination, if\\nthere exists a directed path from A to V that is blocked by a proxy variable and V itself is not a\\nproxy\" [ 82]. They proposed methods to prevent and avoid them. They also show that no observational\\ncriterion can determine whether a predictor exhibits unresolved discrimination; therefore, a causal\\nreasoning framework needs to be incorporated.\\nIn [127], Instead of using the usual risk difference 𝑅𝐷=𝑝1−𝑝2, authors propose a causal risk\\ndifference𝑅𝐷𝑐=𝑝1−𝑝𝑐\\n2for causal discrimination discovery. They define 𝑝𝑐\\n2to be:\\n𝑝𝑐\\n2=Í\\ns∈𝑆,𝑑𝑒𝑐(s)=⊖𝑤(s)\\nÍ\\ns∈𝑆𝑤(s)\\n𝑅𝐷𝑐not close to zero means that there is a bias in decision value due to group membership (causal\\ndiscrimination) or to covariates that have not been accounted for in the analysis (omitted variable\\nbias). This𝑅𝐷𝑐then becomes their causal discrimination measure for discrimination discovery. [ 161]\\nis another work of this type that uses causal networks for discrimination discovery.\\n5.3 Fair Representation Learning\\n5.3.1 Variational Auto Encoders. Learning fair representations and avoiding the unfair interfer-\\nence of sensitive attributes has been introduced in many different research papers. A well-known\\nexample is the Variational Fair Autoencoder introduced in [ 97]. Here,they treat the sensitive variable\\nas the nuisance variable, so that by removing the information about this variable they will get a fair\\nrepresentation. They use a maximum mean discrepancy regularizer to obtain invariance in the poste-\\nrior distribution over latent variables. Adding this maximum mean discrepancy (MMD) penalty into\\nthe lower bound of their V AE architecture satisfies their proposed model for having the Variational\\nFair Autoencoder. Similar work, but not targeting fairness specifically, has been introduced in [ 72].\\nIn [5] authors also propose a debiased V AE architecture called DB-V AE which learns sensitive latent\\nvariables that can bias the model (e.g., skin tone, gender, etc.) and propose an algorithm on top of this\\nDB-V AE using these latent variables to debias systems like facial detection systems. In [ 112] authors\\nmodel their representation-learning task as an optimization objective that would minimize the loss of\\nthe mutual information between the encoding and the sensitive variable. The relaxed version of this\\nassumption is shown in Equation 1. They use this in order to learn fair representation and show that\\nadversarial training is unnecessary and in some cases even counter-productive. In Equation 1, c is the\\nsensitive variable and z the encoding of x.\\n𝑚𝑖𝑛\\n𝑞L(𝑞,𝑥)+𝜆𝐼(𝑧,𝑐) (1)\\nIn [42], authors introduce flexibly fair representation learning by disentanglement that disentangles\\ninformation from multiple sensitive attributes. Their flexible and fair variational autoencoder is\\nnot only flexible with respect to downstream task labels but also flexible with respect to sensitive\\nattributes. They address the demographic parity notion of fairness, which can target multiple sensitive\\nattributes or any subset combination of them.\\n5.3.2 Adversarial Learning. In [90] authors present a framework to mitigate bias in models\\nlearned from data with stereotypical associations. They propose a model in which they are trying to\\nmaximize accuracy of the predictor on y, and at the same time minimize the ability of the adversary\\nto predict the protected or sensitive variable (stereotyping variable z). The model consists of two\\nparts—the predictor and the adversary—as shown in Figure 6. In their model, the predictor is trained\\nto predict Y given X. With the help of a gradient-based approach like stochastic gradient descent,\\nthe model tries to learn the weights W by minimizing some loss function LP( ˆ𝑦, y). The output layer\\nis passed to an adversary, which is another network. This network tries to predict Z. The adversary'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 19}, page_content='20 Mehrabi et al.\\nmay have different inputs depending on the fairness definition needing to be achieved. For instance,\\nin order to satisfy Demographic Parity , the adversary would try to predict the protected variable Z\\nusing only the predicted label ˆ𝑌passed as an input to it, while preventing the adversary from learning\\nthis is the goal of the predictor. Similarly, to achieve Equality of Odds , the adversary would get\\nthe true label Y in addition to the predicted label ˆ𝑌. To satisfy Equality of Opportunity for a given\\nclass y, they would only select instances for the adversary where Y=y. [ 156] takes an interesting\\nand different direction toward solving fairness issues using adversarial networks by introducing\\nFairGAN which generates synthetic data that is free from discrimination and is similar to the real\\ndata. They use their newly generated synthetic data from FairGAN, which is now debiased, instead\\nof the real data for training and testing. They do not try to remove discrimination from the dataset,\\nunlike many of the existing approaches, but instead generate new datasets similar to the real one\\nwhich is debiased and preserves good data utility. The architecture of their FairGAN model is shown\\nin Figure 5. FairGAN consists of two components: a generator 𝐺𝐷𝑒𝑐which generates the fake data\\nconditioned on the protected attribute 𝑃𝐺(𝑥,𝑦,𝑠)=𝑃𝐺(𝑥,𝑦|𝑠)𝑃𝐺(𝑠)where𝑃𝐺(𝑠)=𝑃𝑑𝑎𝑡𝑎(𝑠), and two\\ndiscriminators 𝐷1and𝐷2.𝐷1is trained to differentiate the real data denoted by 𝑃𝑑𝑎𝑡𝑎(𝑥,𝑦,𝑠)from\\nthe generated fake data denoted by 𝑃𝐺(𝑥,𝑦,𝑠).\\nPzNoisePsProtected AttributeGDecGeneratorPG(!,#|%)Pdata(!,#|%)D2DiscriminatorD1DiscriminatorReal: (!,#,%)Fake: (&!,&#,̂%)(&!,&#|̂%= 0)(&!,&#|̂%= 1)\\nFig. 5. Structure of FairGAN as proposed in [156].\\nFig. 6. The architecture of adversarial network proposed in [90]©Brian Hu Zhang.\\nIn addition to that, for achieving fairness constraints, such as statistical parity, 𝑃𝐺(𝑥,𝑦|𝑠=1)=\\n𝑃𝐺(𝑥,𝑦|𝑠=0), the training of 𝐷2is such that it emphasizes differentiation of the two types of'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 20}, page_content='A Survey on Bias and Fairness in Machine Learning 21\\nsynthetic (generated by the model) samples 𝑃𝐺(𝑥,𝑦|𝑠=1)and𝑃𝐺(𝑥,𝑦|𝑠=0)indicating if the\\nsynthetic samples are from the unprotected or protected groups. Here s denotes the protected or the\\nsensitive variable, and we adapted the same notation as in [156].\\n5.4 Fair NLP\\n5.4.1 Word Embedding. In [20] authors noticed that while using state-of-the-art word embeddings\\nin word analogy tests, “man” would be mapped to “computer programmer” and “woman” would\\nbe mapped to “homemaker.” This bias toward woman triggered the authors to propose a method to\\ndebias word embeddings by proposing a method that respects the embeddings for gender-specific\\nwords but debiases embeddings for gender-neutral words by following these steps: (Notice that Step\\n2 has two different options. Depending on whether you target hard debiasing or soft debiasing, you\\nwould use either step 2a or 2b)\\n(1)Identify gender subspace. Identifying a direction of the embedding that captures the bias\\n[20].\\n(2)Hard debiasing or soft debiasing:\\n(a)Hard debiasing (neutralize and equalize). Neutralize puts away the gender subspace\\nfrom gender-neutral words and makes sure that all the gender-neutral words are removed\\nand zeroed out in the gender subspace [ 20]. Equalize makes gender-neutral words to be\\nequidistant from the equality set of gendered words [20].\\n(b)Soft bias correction. Tries to move as little as possible to retain its similarity to the original\\nembedding as much as possible, while reducing the gender bias. This trade-off is controlled\\nby a parameter [20].\\nFollowing on the footsteps of these authors, other future work attempted to tackle this problem\\n[169] by generating a gender-neutral version of (Glove called GN-Glove) that tries to retain gender\\ninformation in some of the word embedding’s learned dimensions, while ensuring that other dimen-\\nsions are free from this gender effect. This approach primarily relies on Glove as its base model\\nwith gender as the protected attribute. However, a recent paper [ 58] argues against these debiasing\\ntechniques and states that many recent works on debiasing word embeddings have been superficial,\\nthat those techniques just hide the bias and don’t actually remove it. A recent work [23] took a new\\ndirection and proposed a preprocessing method for the discovery of the problematic documents in\\nthe training corpus that have biases in them, and tried to debias the system by perturbing or removing\\nthese documents efficiently from the training corpus. In a very recent work [ 166], authors target bias\\nin ELMo’s contextualized word vectors and attempt to analyze and mitigate the observed bias in the\\nembeddings. They show that the corpus used for training of ELMo has a significant gender skew,\\nwith male entities being nearly three times more common than female entities. This automatically\\nleads to gender bias in these pretrained contextualized embeddings. They propose the following two\\nmethods for mitigating the existing bias while using the pretrained embeddings in a downstream task,\\ncoreference resolution: (1) train-time data augmentation approach, and (2) test-time neutralization\\napproach.\\n5.4.2 Coreference Resolution. The [ 168] paper shows that coreference systems have a gender\\nbias. They introduce a benchmark, called WinoBias, focusing on gender bias in coreference resolution.\\nIn addition to that, they introduce a data-augmentation technique that removes bias in the existing\\nstate-of-the-art coreferencing methods, in combination with using word2vec debiasing techniques.\\nTheir general approach is as follows: They first generate auxiliary datasets using a rule-based\\napproach in which they replace all the male entities with female entities and the other way around.\\nThen they train models with a combination of the original and the auxiliary datasets. They use the\\nabove solution in combination with word2vec debiasing techniques to generate word embeddings.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 21}, page_content='22 Mehrabi et al.\\nThey also point out sources of gender bias in coreference systems and propose solutions to them.\\nThey show that the first source of bias comes from the training data and propose a solution that\\ngenerates an auxiliary data set by swapping male and female entities. Another case arises from\\nthe resource bias (word embeddings are bias), so the proposed solution is to replace Glove with a\\ndebiased embedding method. Last, another source of bias can come from unbalanced gender lists, and\\nbalancing the counts in the lists is a solution they proposed. In another work [ 134], authors also show\\nthe existence of gender bias in three state-of-the-art coreference resolution systems by observing that\\nfor many occupations, these systems resolve pronouns in a biased fashion by preferring one gender\\nover the other.\\n5.4.3 Language Model. In [21] authors introduce a metric for measuring gender bias in a\\ngenerated text from a language model based on recurrent neural networks that is trained on a text\\ncorpus along with measuring the bias in the training text itself. They use Equation 2, where 𝑤is any\\nword in the corpus, 𝑓is a set of gendered words that belong to the female category, such as she, her,\\nwoman, etc., and 𝑚to the male category, and measure the bias using the mean absolute and standard\\ndeviation of the proposed metric along with fitting a univariate linear regression model over it and\\nthen analyzing the effectiveness of each of those metrics while measuring the bias.\\n𝑏𝑖𝑎𝑠(𝑤)=𝑙𝑜𝑔(𝑃(𝑤|𝑓)\\n𝑃(𝑤|𝑚)) (2)\\nIn their language model, they also introduce a regularization loss term that would minimize the\\nprojection of embeddings trained by the encoder onto the embedding of the gender subspace following\\nthe soft debiasing technique introduced in [ 20]. Finally, they evaluate the effectiveness of their method\\non reducing gender bias and conclude by stating that in order to reduce bias, there is a compromise\\non perplexity. They also point out the effectiveness of word-level bias metrics over the corpus-level\\nmetrics.\\n5.4.4 Sentence Encoder. In [100] authors extend the research in detecting bias in word embed-\\nding techniques to that of sentence embedding. They try to generalize bias-measuring techniques,\\nsuch as using the Word Embedding Association Test (WEAT [ 26]) in the context of sentence en-\\ncoders by introducing their new sentence encoding bias-measuring techniques, the Sentence Encoder\\nAssociation Test (SEAT). They used state-of-the-art sentence encoding techniques, such as CBoW,\\nGPT, ELMo, and BERT, and find that although there was varying evidence of human-like bias in\\nsentence encoders using SEAT, more recent methods like BERT are more immune to biases. That\\nbeing said, they are not claiming that these models are bias-free, but state that more sophisticated\\nbias discovery techniques may be used in these cases, thereby encouraging more future work in this\\narea.\\n5.4.5 Machine Translation. In [52] authors noticed that when translating the word \"friend\" in the\\nfollowing two sentences from English to Spanish, they achieved different results—although in both\\ncases this word should be translated the same way.\\n\"She works in a hospital, my friend is a nurse.\"\\n\"She works in a hospital, my friend is a doctor.\"\\nIn both of these sentences, \"friend\" should be translated to the female version of Spanish friend\\n\"amiga,\" but the results were not reflecting this expectation. For the second sentence, friend was\\ntranslated to \"amigo,\"—the male version of friend in Spanish. This is because doctor is more\\nstereotypical to males and nurse to females, and the model picks this bias or stereotype and reflects it\\nin its performance. To solve this, authors in [ 52] build an approach that leverages the fact that machine\\ntranslation uses word embeddings. They use the existing debiasing methods in word embedding and\\napply them in the machine translation pipeline. This not only helped them to mitigate the existing bias'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 22}, page_content='A Survey on Bias and Fairness in Machine Learning 23\\nin their system, but also boosted the performance of their system by one BLUE score. In [ 126] authors\\nshow that Google’s translate system can suffer from gender bias by making sentences taken from the\\nU.S. Bureau of Labor Statistics into a dozen languages that are gender neutral, including Yoruba,\\nHungarian, and Chinese, translating them into English, and showing that Google Translate shows\\nfavoritism toward males for stereotypical fields such as STEM jobs. In [ 148] authors annotated and\\nanalyzed the Europarl dataset [ 84], a large political, multilingual dataset used in machine translation,\\nand discovered that with the exception of the youngest age group (20-30), which represents only a\\nvery small percentage of the total amount of sentences (0.71%), more male data is available in all age\\ngroups. They also looked at the entire dataset and showed that 67.39% of the sentences are produced\\nby male speakers. Furthermore, to mitigate the gender-related issues and to improve morphological\\nagreement in machine translation, they augmented every sentence with a tag on the English source\\nside, identifying the gender of the speaker. This helped the system in most of the cases, but not\\nalways, so further work has been suggested for integrating speaker information in other ways.\\n5.4.6 Named Entity Recognition. In [101], authors investigate a type of existing bias in various\\nnamed entity recognition (NER) systems. In particular, they observed that in a context where an\\nentity should be tagged as a person entity, such as \"John is a person\" or \"John is going to school\",\\nmore female names as opposed to male names are being tagged as non-person entities or not being\\ntagged at all. To further formalize their observations, authors propose six different evaluation metrics\\nthat would measure amount of bias among different genders in NER systems. They curated templated\\nsentences pertaining to human actions and applied these metrics on names from U.S census data\\nincorporated into the templates. The six introduced measures each aim to demonstrate a certain type\\nof bias and serve a specific purpose in showing various results as follows:\\n•Error Type-1 Unweighted: Through this type of error, authors wanted to recognize the pro-\\nportion of entities that are tagged as anything other than the person entity in each of the male\\nvs female demographic groups. This could be the entity not being tagged or tagged as other\\nentities, such as location.\\nÍ\\n𝑛∈𝑁𝑓𝐼(𝑛𝑡𝑦𝑝𝑒≠𝑃𝐸𝑅𝑆𝑂𝑁)\\n|𝑁𝑓|\\n•Error Type-1 Weighted: This type of error is similar to its unweighted case except authors\\nconsidered the frequency or popularity of names so that they could penalize if a more popular\\nname is being tagged wrongfully.\\nÍ\\n𝑛∈𝑁𝑓𝑓𝑟𝑒𝑞𝑓(𝑛𝑡𝑦𝑝𝑒≠𝑃𝐸𝑅𝑆𝑂𝑁)\\nÍ\\n𝑛∈𝑁𝑓𝑓𝑟𝑒𝑞𝑓(𝑛),\\nwhere𝑓𝑟𝑒𝑞𝑓(·)indicates the frequency of a name for a particular year in the female census\\ndata. Likewise, 𝑓𝑟𝑒𝑞𝑚(·)indicates the frequency of a name for a particular year in the male\\ncensus data.\\n•Error Type-2 Unweighted: This is a type of error in which the entity is tagged as other entities,\\nsuch as location or city. Notice that this error does not count if the entity is not tagged.\\nÍ\\n𝑛∈𝑁𝑓𝐼(𝑛𝑡𝑦𝑝𝑒∉{∅,𝑃𝐸𝑅𝑆𝑂𝑁})\\n|𝑁𝑓|,\\nwhere∅indicates that the name is not tagged.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 23}, page_content='24 Mehrabi et al.\\n•Error Type-2 Weighted: This error is again similar to its unweighted case except the frequency\\nis taken into consideration.Í\\n𝑛∈𝑁𝑓𝑓𝑟𝑒𝑞𝑓(𝑛𝑡𝑦𝑝𝑒∉{∅,𝑃𝐸𝑅𝑆𝑂𝑁})\\nÍ\\n𝑛∈𝑁𝑓𝑓𝑟𝑒𝑞𝑓(𝑛)\\n•Error Type-3 Unweighted: This is a type of error in which it reports if the entity is not tagged\\nat all. Notice that even if the entity is tagged as a non-person entity this error type would not\\nconsider it.Í\\n𝑛∈𝑁𝑓𝐼(𝑛𝑡𝑦𝑝𝑒=∅)\\n|𝑁𝑓|\\n•Error Type-3 Weighted: Again, this error is similar to its unweighted case with frequency taken\\ninto consideration.Í\\n𝑛∈𝑁𝑓𝑓𝑟𝑒𝑞𝑓(𝑛𝑡𝑦𝑝𝑒=∅)\\nÍ\\n𝑛∈𝑁𝑓𝑓𝑟𝑒𝑞𝑓(𝑛)\\nAuthors also investigate the data that these NER systems are trained on and find that the data is also\\nbiased toward female gender by not including as versatile names as there should be to represent\\nfemale names.\\n5.5 Comparison of Different Mitigation Algorithms\\nThe field of algorithmic fairness is a relatively new area of research and work still needs to be done\\nfor its improvement. With that being said, there are already papers that propose fair AI algorithms and\\nbias mitigation techniques and compare different mitigation algorithms using different benchmark\\ndatasets in the fairness domain. For instance, authors in [ 65] propose a geometric solution to learn fair\\nrepresentations that removes correlation between protected and unprotected features. The proposed\\napproach can control the trade-off between fairness and accuracy via an adjustable parameter. In\\nthis work, authors evaluate the performance of their approach on different benchmark datasets, such\\nas COMPAS, Adult and German, and compare them against various different approaches for fair\\nlearning algorithms considering fairness and accuracy measures [ 65,72,158,159]. In addition,\\nIBM’s AI Fairness 360 (AIF360) toolkit [ 11] has implemented many of the current fair learning\\nalgorithms and has demonstrated some of the results as demos which can be utilized by interested\\nusers to compare different methods with regards to different fairness measures.\\n6 CHALLENGES AND OPPORTUNITIES FOR FAIRNESS RESEARCH\\nWhile there have been many definitions of, and approaches to, fairness in the literature, the study\\nin this area is anything but complete. Fairness and algorithmic bias still holds a number of research\\nopportunities. In this section, we provide pointers to outstanding challenges in fairness research, and\\nan overview of opportunities for development of understudied problems.\\n6.1 Challenges\\nThere are several remaining challenges to be addressed in the fairness literature. Among them are:\\n(1)Synthesizing a definition of fairness. Several definitions of what would constitute fairness\\nfrom a machine learning perspective have been proposed in the literature. These definitions\\ncover a wide range of use cases, and as a result are somewhat disparate in their view of fairness.\\nBecause of this, it is nearly impossible to understand how one fairness solution would fare\\nunder a different definition of fairness. Synthesizing these definitions into one remains an open\\nresearch problem since it can make evaluation of these systems more unified and comparable.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 24}, page_content='A Survey on Bias and Fairness in Machine Learning 25\\nhaving a more unified fairness definition and framework can also help with the incompatibility\\nissue of some current fairness definitions.\\n(2)From Equality to Equity. The definitions presented in the literature mostly focus on equality ,\\nensuring that each individual or group is given the same amount of resources, attention or\\noutcome. However, little attention has been paid to equity , which is the concept that each\\nindividual or group is given the resources they need to succeed [ 60,103]. Operationalizing this\\ndefinition and studying how it augments or contradicts existing definitions of fairness remains\\nan exciting future direction.\\n(3)Searching for Unfairness. Given a definition of fairness, it should be possible to identify\\ninstances of this unfairness in a particular dataset. Inroads toward this problem have been made\\nin the areas of data bias by detecting instances of Simpson’s Paradox in arbitrary datasets [ 3];\\nhowever, unfairness may require more consideration due to the variety of definitions and the\\nnuances in detecting each one.\\nFig. 7. Heatmap depicting distribution of previous work in fairness, grouped by domain and fairness\\ndefinition.\\n6.2 Opportunities\\nIn this work we have taxonomized and summarized the current state of research into algorithmic\\nbiases and fairness—with a particular focus on machine learning. Even in this area alone, the research\\nis broad. Subareas, from natural language processing, to representation learning, to community\\ndetection, have all seen efforts to make their methodologies more fair. Nevertheless, every area\\nhas not received the same amount of attention from the research community. Figure 7 provides an\\noverview of what has been done in different areas to address fairness—categorized by the fairness\\ndefinition type and domain. Some areas (e.g., community detection at the subgroup level) have\\nreceived no attention in the literature, and could be fertile future research areas.\\n7 CONCLUSION\\nIn this survey we introduced problems that can adversely affect AI systems in terms of bias and\\nunfairness. The issues were viewed primarily from two dimensions: data and algorithms. We illus-\\ntrated problems that demonstrate why fairness is an important issue. We further showed examples of'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 25}, page_content='26 Mehrabi et al.\\nthe potential real-world harm that unfairness can have on society—such as applications in judicial\\nsystems, face recognition, and promoting algorithms. We then went over the definitions of fairness\\nand bias that have been proposed by researchers. To further stimulate the interest of readers, we\\nprovided some of the work done in different areas in terms of addressing the biases that may affect AI\\nsystems and different methods and domains in AI, such as general machine learning, deep learning\\nand natural language processing. We then further subdivided the fields into a more fine-grained\\nanalysis of each subdomain and the work being done to address fairness constraints in each. The\\nhope is to expand the horizons of the readers to think deeply while working on a system or a method\\nto ensure that it has a low likelihood of causing potential harm or bias toward a particular group.\\nWith the expansion of AI use in our world, it is important that researchers take this issue seriously\\nand expand their knowledge in this field. In this survey we categorized and created a taxonomy of\\nwhat has been done so far to address different issues in different domains regarding the fairness issue.\\nOther possible future work and directions can be taken to address the existing problems and biases in\\nAI that we discussed in the previous sections.\\n8 ACKNOWLEDGMENTS\\nThis material is based upon work supported by the Defense Advanced Research Projects Agency\\n(DARPA) under Agreement No. HR0011890019. We would like to thank the organizers, speakers\\nand the attendees at the IV ADO-Mila 2019 Summer School on Bias and Discrimination in AI. We\\nwould like to also thank Brian Hu Zhang and Shreya Shankar.\\n9 APPENDIX\\n9.1 Datasets for Fairness Research\\nAside from the existence of bias in datasets, there are datasets that are specifically used to address\\nbias and fairness issues in machine learning. There are also some datasets that are introduced to\\ntarget the issues and biases previously observed in older existing datasets. Below we list some of the\\nwidely known datasets that have the characteristics discussed in this survey.\\n9.1.1 UCI Adult Dataset. UCI Adult dataset, also known as \"Census Income\" dataset, contains\\ninformation, extracted from the 1994 census data about people with attributes such as age, occupation,\\neducation, race, sex, marital-status, native-country, hours-per-week etc., indicating whether the\\nincome of a person exceeds $50K/yr or not. It can be used in fairness-related studies that want to\\ncompare gender or race inequalities based on people’s annual incomes, or various other studies [7].\\n9.1.2 German Credit Dataset. The German Credit dataset contains 1000 credit records contain-\\ning attributes such as personal status and sex, credit score, credit amount, housing status etc. It can\\nbe used in studies about gender inequalities on credit-related issues [47].\\n9.1.3 WinoBias. The WinoBias dataset follows the winograd format and has 40 occupations in\\nsentences that are referenced to human pronouns. There are two types of challenge sentences in the\\ndataset requiring linkage of gendered pronouns to either male or female stereotypical occupations. It\\nwas used in the coreference resolution study to certify if a system has gender bias or not—in this\\ncase, towards stereotypical occupations [168].\\n9.1.4 Communities and Crime Dataset. The Communities and Crime dataset gathers infor-\\nmation from different communities in the United States related to several factors that can highly\\ninfluence some common crimes such as robberies, murders or rapes. The data includes crime data\\nobtained from the 1990 US LEMAS survey and the 1995 FBI Unified Crime Report. It also contains\\nsocio-economic data from the 1990 US Census.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 26}, page_content='A Survey on Bias and Fairness in Machine Learning 27\\n9.1.5 COMPAS Dataset. The COMPAS dataset contains records for defendants from Broward\\nCounty indicating their jail and prison times, demographics, criminal histories, and COMPAS risk\\nscores from 2013 to 2014 [89].\\n9.1.6 Recidivism in Juvenile Justice Dataset. The Recidivism in Juvenile Justice dataset\\ncontains all juvenile offenders between ages 12-17 who committed a crime between years 2002 and\\n2010 and completed a prison sentence in 2010 in Catalonia’s juvenile justice system [145].\\n9.1.7 Pilot Parliaments Benchmark Dataset. The Pilot Parliaments Benchmark dataset, also\\nknown as PPB, contains images of 1270 individuals in the national parliaments from three European\\n(Iceland, Finland, Sweden) and three African (Rwanda, Senegal, South Africa) countries. This\\nbenchmark was released to have more gender and race balance, diversity, and representativeness\\n[24].\\n9.1.8 Diversity in Faces Dataset. The Diversity in Faces (DiF) is an image dataset collected for\\nfairness research in face recognition. DiF is a large dataset containing one million annotations for\\nface images. It is also a diverse dataset with diverse facial features, such as different craniofacial\\ndistances, skin color, facial symmetry and contrast, age, pose, gender, resolution, along with diverse\\nareas and ratios [107].\\nDataset Name Reference Size Area\\nUCI adult dataset [7] 48,842 income records Social\\nGerman credit dataset [47] 1,000 credit records Financial\\nPilot parliaments benchmark dataset [24] 1,270 images Facial images\\nWinoBias [168] 3,160 sentences Coreference resolution\\nCommunities and crime dataset [129] 1,994 crime records Social\\nCOMPAS Dataset [89] 18,610 crime records Social\\nRecidivism in juvenile justice dataset [28] 4,753 crime records Social\\nDiversity in faces dataset [107] 1 million images Facial images\\nTable 5. Most widely used datasets in the fairness domain with additional information about each of\\nthe datasets including their size and area of concentration.\\nREFERENCES\\n[1]Alekh Agarwal, Miroslav Dudik, and Zhiwei Steven Wu. 2019. Fair Regression: Quantitative Definitions and Reduction-\\nBased Algorithms. In International Conference on Machine Learning . 120–129.\\n[2]Sina Aghaei, Mohammad Javad Azizi, and Phebe Vayanos. 2019. Learning optimal and fair decision trees for non-\\ndiscriminative decision-making. In Proceedings of the AAAI Conference on Artificial Intelligence , V ol. 33. 1418–1426.\\n[3]Nazanin Alipourfard, Peter G Fennell, and Kristina Lerman. 2018. Can you Trust the Trend?: Discovering Simpson’s\\nParadoxes in Social Data. In Proceedings of the Eleventh ACM International Conference on Web Search and Data\\nMining . ACM, 19–27.\\n[4]Nazanin Alipourfard, Peter G Fennell, and Kristina Lerman. 2018. Using Simpson’s Paradox to Discover Interesting\\nPatterns in Behavioral Data. In Twelfth International AAAI Conference on Web and Social Media .\\n[5]Alexander Amini, Ava Soleimany, Wilko Schwarting, Sangeeta Bhatia, and Daniela Rus. 2019. Uncovering and\\nMitigating Algorithmic Bias through Learned Latent Structure. (2019).\\n[6]Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine Bias: there’s software used across the\\ncountry to predict future criminals. And it’s biased against blacks. ProPublica 2016.\\n[7]A. Asuncion and D.J. Newman. 2007. UCI Machine Learning Repository. http://www.ics.uci.edu/$\\\\sim$mlearn/\\n{MLR}epository.html\\n[8]Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. 2019. Scalable Fair\\nClustering. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine\\nLearning Research, Vol. 97) , Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, Long Beach, California,\\nUSA, 405–413. http://proceedings.mlr.press/v97/backurs19a.html'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 27}, page_content='28 Mehrabi et al.\\n[9]Ricardo Baeza-Yates. 2018. Bias on the Web. Commun. ACM 61, 6 (May 2018), 54–61. https://doi.org/10.1145/\\n3209581\\n[10] Samuel Barbosa, Dan Cosley, Amit Sharma, and Roberto M. Cesar-Jr. 2016. Averaging Gone Wrong: Using Time-\\nAware Analyses to Better Understand Behavior. (April 2016), 829–841.\\n[11] Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay\\nLohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al .2018. Ai fairness 360: An extensible toolkit\\nfor detecting, understanding, and mitigating unwanted algorithmic bias. arXiv preprint arXiv:1810.01943 (2018).\\n[12] Emily M. Bender and Batya Friedman. 2018. Data Statements for Natural Language Processing: Toward Mitigating\\nSystem Bias and Enabling Better Science. Transactions of the Association for Computational Linguistics 6 (2018),\\n587–604. https://doi.org/10.1162/tacl_a_00041\\n[13] Misha Benjamin, Paul Gagnon, Negar Rostamzadeh, Chris Pal, Yoshua Bengio, and Alex Shee. [n.d.]. TOWARDS\\nSTANDARDIZATION OF DATA LICENSES: THE MONTREAL DATA LICENSE. ([n. d.]).\\n[14] Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and\\nAaron Roth. 2017. A Convex Framework for Fair Regression. arXiv:1706.02409 [cs.LG]\\n[15] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. [n.d.]. Fairness in criminal justice risk\\nassessments: The state of the art. Sociological Methods & Research ([n. d.]), 0049124118782533.\\n[16] Peter J Bickel, Eugene A Hammel, and J William O’Connell. 1975. Sex bias in graduate admissions: Data from\\nBerkeley. Science 187, 4175 (1975), 398–404.\\n[17] RDP Binns. 2018. Fairness in machine learning: Lessons from political philosophy. Journal of Machine Learning\\nResearch (2018).\\n[18] Colin R Blyth. 1972. On Simpson’s paradox and the sure-thing principle. J. Amer. Statist. Assoc. 67, 338 (1972),\\n364–366.\\n[19] Miranda Bogen and Aaron Rieke. 2018. Help wanted: an examination of hiring algorithms, equity . Technical Report.\\nand bias. Technical report, Upturn.\\n[20] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer\\nprogrammer as woman is to homemaker? debiasing word embeddings. In Advances in neural information processing\\nsystems . 4349–4357.\\n[21] Shikha Bordia and Samuel Bowman. 2019. Identifying and Reducing Gender Bias in Word-Level Language Models. In\\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:\\nStudent Research Workshop . 7–15.\\n[22] Avishek Bose and William Hamilton. 2019. Compositional Fairness Constraints for Graph Embeddings. In International\\nConference on Machine Learning . 715–724.\\n[23] Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. 2019. Understanding the\\nOrigins of Bias in Word Embeddings. In Proceedings of the 36th International Conference on Machine Learning\\n(Proceedings of Machine Learning Research, Vol. 97) , Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR,\\nLong Beach, California, USA, 803–811. http://proceedings.mlr.press/v97/brunet19a.html\\n[24] Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender\\nClassification. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency (Proceedings of\\nMachine Learning Research, Vol. 81) , Sorelle A. Friedler and Christo Wilson (Eds.). PMLR, New York, NY , USA,\\n77–91. http://proceedings.mlr.press/v81/buolamwini18a.html\\n[25] Toon Calders and Sicco Verwer. 2010. Three naive Bayes approaches for discrimination-free classification. Data\\nMining and Knowledge Discovery 21, 2 (2010), 277–292.\\n[26] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora\\ncontain human-like biases. Science 356, 6334 (2017), 183–186.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 27}, page_content='[26] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora\\ncontain human-like biases. Science 356, 6334 (2017), 183–186.\\n[27] Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R Varshney. 2017.\\nOptimized Pre-Processing for Discrimination Prevention. In Advances in Neural Information Processing Systems 30 ,\\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates,\\nInc., 3992–4001. http://papers.nips.cc/paper/6988-optimized-pre-processing-for-discrimination-prevention.pdf\\n[28] Manel Capdevila, Marta Ferrer, and Eulália Luque. 2005. La reincidencia en el delito en la justicia de menores. Centro\\nde estudios jurídicos y formación especializada, Generalitat de Catalunya. Documento no publicado (2005).\\n[29] Allison JB Chaney, Brandon M Stewart, and Barbara E Engelhardt. 2018. How algorithmic confounding in recom-\\nmendation systems increases homogeneity and decreases utility. In Proceedings of the 12th ACM Conference on\\nRecommender Systems . ACM, 224–232.\\n[30] Jiahao Chen, Nathan Kallus, Xiaojie Mao, Geoffry Svacha, and Madeleine Udell. 2019. Fairness Under Unawareness:\\nAssessing Disparity When Protected Class Is Unobserved. In Proceedings of the Conference on Fairness, Accountability,\\nand Transparency . ACM, 339–348.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 28}, page_content='A Survey on Bias and Fairness in Machine Learning 29\\n[31] Xingyu Chen, Brandon Fain, Liang Lyu, and Kamesh Munagala. 2019. Proportionally Fair Clustering. In International\\nConference on Machine Learning . 1032–1041.\\n[32] S. Chiappa. 2019. Path-specific Counterfactual Fairness. In Thirty-Third AAAI Conference on Artificial Intelligence .\\n7801–7808.\\n[33] S. Chiappa and W. S. Isaac. 2019. A Causal Bayesian Networks Viewpoint on Fairness. In E. Kosta, J. Pierson,\\nD. Slamanig, S. Fischer-Hübner, S. Krenn (eds) Privacy and Identity Management. Fairness, Accountability, and\\nTransparency in the Age of Big Data. Privacy and Identity 2018. IFIP Advances in Information and Communication\\nTechnology , V ol. 547. Springer, Cham.\\n[34] Alexandra Chouldechova. 2017. Fair prediction with disparate impact: A study of bias in recidivism prediction\\ninstruments. Big data 5, 2 (2017), 153–163.\\n[35] Alexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. 2018. A case study\\nof algorithm-assisted decision making in child maltreatment hotline screening decisions. In Proceedings of the 1st\\nConference on Fairness, Accountability and Transparency (Proceedings of Machine Learning Research, Vol. 81) ,\\nSorelle A. Friedler and Christo Wilson (Eds.). PMLR, New York, NY , USA, 134–148. http://proceedings.mlr.press/\\nv81/chouldechova18a.html\\n[36] Alexandra Chouldechova and Aaron Roth. 2018. The frontiers of fairness in machine learning. arXiv preprint\\narXiv:1810.08810 (2018).\\n[37] John S Chuang, Olivier Rivoire, and Stanislas Leibler. 2009. Simpson’s paradox in a synthetic microbial system.\\nScience 323, 5911 (2009), 272–275.\\n[38] Kevin A Clarke. 2005. The phantom menace: Omitted variable bias in econometric research. Conflict management and\\npeace science 22, 4 (2005), 341–352.\\n[39] Lee Cohen, Zachary C. Lipton, and Yishay Mansour. 2019. Efficient candidate screening under multiple tests and\\nimplications for fairness. arXiv:1905.11361 [cs.LG]\\n[40] United States. Equal Employment Opportunity Commission. [n.d.]. EEOC compliance manual .[Washington, D.C.]:\\nU.S. Equal Employment Opportunity Commission, [1992].\\n[41] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic decision making and\\nthe cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and\\nData Mining . ACM, 797–806.\\n[42] Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard\\nZemel. 2019. Flexibly Fair Representation Learning by Disentanglement. In International Conference on Machine\\nLearning . 1436–1445.\\n[43] Brian d’Alessandro, Cathy O’Neil, and Tom LaGatta. 2017. Conscientious classification: A data scientist’s guide to\\ndiscrimination-aware classification. Big data 5, 2 (2017), 120–134.\\n[44] David Danks and Alex John London. 2017. Algorithmic Bias in Autonomous Systems.. In IJCAI . 4691–4697.\\n[45] Shai Danziger, Jonathan Levav, and Liora Avnaim-Pesso. 2011. Extraneous factors in judicial decisions. Proceedings\\nof the National Academy of Sciences 108, 17 (2011), 6889–6892.\\n[46] Julia Dressel and Hany Farid. 2018. The accuracy, fairness, and limits of predicting recidivism. Science Advances 4, 1\\n(2018). https://doi.org/10.1126/sciadv.aao5580 arXiv:https://advances.sciencemag.org/content/4/1/eaao5580.full.pdf\\n[47] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml\\n[48] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness Through Awareness.\\nInProceedings of the 3rd Innovations in Theoretical Computer Science Conference (Cambridge, Massachusetts) (ITCS\\n’12). ACM, New York, NY , USA, 214–226. https://doi.org/10.1145/2090236.2090255\\n[49] Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, and Max Leiserson. 2018. Decoupled Classifiers for\\nGroup-Fair and Efficient Machine Learning. In Proceedings of the 1st Conference on Fairness, Accountability and'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 28}, page_content='Group-Fair and Efficient Machine Learning. In Proceedings of the 1st Conference on Fairness, Accountability and\\nTransparency (Proceedings of Machine Learning Research, Vol. 81) , Sorelle A. Friedler and Christo Wilson (Eds.).\\nPMLR, New York, NY , USA, 119–133. http://proceedings.mlr.press/v81/dwork18a.html\\n[50] Golnoosh Farnadi, Behrouz Babaki, and Lise Getoor. 2018. Fairness in Relational Domains. In Proceedings of the\\n2018 AAAI/ACM Conference on AI, Ethics, and Society (New Orleans, LA, USA) (AIES ’18) . ACM, New York, NY ,\\nUSA, 108–114. https://doi.org/10.1145/3278721.3278733\\n[51] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. 2015.\\nCertifying and Removing Disparate Impact. In Proceedings of the 21th ACM SIGKDD International Conference on\\nKnowledge Discovery and Data Mining (Sydney, NSW, Australia) (KDD ’15) . Association for Computing Machinery,\\nNew York, NY , USA, 259–268. https://doi.org/10.1145/2783258.2783311\\n[52] Joel Escudé Font and Marta R Costa-jussà. 2019. Equalizing Gender Biases in Neural Machine Translation with Word\\nEmbeddings Techniques. arXiv preprint arXiv:1901.03116 (2019).\\n[53] Batya Friedman and Helen Nissenbaum. 1996. Bias in Computer Systems. ACM Trans. Inf. Syst. 14, 3 (July 1996),\\n330–347. https://doi.org/10.1145/230538.230561'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 29}, page_content='30 Mehrabi et al.\\n[54] Anna Fry, Thomas J Littlejohns, Cathie Sudlow, Nicola Doherty, Ligia Adamska, Tim Sprosen, Rory Collins, and\\nNaomi E Allen. 2017. Comparison of Sociodemographic and Health-Related Characteristics of UK Biobank Participants\\nWith Those of the General Population. American Journal of Epidemiology 186, 9 (06 2017), 1026–1034. https:\\n//doi.org/10.1093/aje/kwx246 arXiv:http://oup.prod.sis.lan/aje/article-pdf/186/9/1026/24330720/kwx246.pdf\\n[55] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and\\nKate Crawford. [n.d.]. Datasheets for Datasets. ([n. d.]).\\n[56] C. E. Gehlke and Katherine Biehl. 1934. Certain effects of grouping upon the size of the correlation coefficient in\\ncensus tract material. J. Amer. Statist. Assoc. 29, 185A (1934), 169–170. https://doi.org/10.2307/2277827\\n[57] Naman Goel, Mohammad Yaghini, and Boi Faltings. 2018. Non-discriminatory machine learning through convex\\nfairness criteria. In Thirty-Second AAAI Conference on Artificial Intelligence .\\n[58] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in\\nWord Embeddings But do not Remove Them. arXiv preprint arXiv:1903.03862 (2019).\\n[59] Sandra González-Bailón, Ning Wang, Alejandro Rivero, Javier Borge-Holthoefer, and Yamir Moreno. 2014. Assessing\\nthe bias in samples of large online networks. Social Networks 38 (2014), 16–27.\\n[60] Susan T Gooden. 2015. Race and social equity: A nervous area of government . Routledge.\\n[61] Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian Weller. 2016. The case for process\\nfairness in learning: Feature selection for fair decision making. In NIPS Symposium on Machine Learning and the Law ,\\nV ol. 1. 2.\\n[62] S. Hajian and J. Domingo-Ferrer. 2013. A Methodology for Direct and Indirect Discrimination Prevention in\\nData Mining. IEEE Transactions on Knowledge and Data Engineering 25, 7 (July 2013), 1445–1459. https:\\n//doi.org/10.1109/TKDE.2012.72\\n[63] Moritz Hardt, Eric Price, Nati Srebro, et al .2016. Equality of opportunity in supervised learning. In Advances in neural\\ninformation processing systems . 3315–3323.\\n[64] Eszter Hargittai. 2007. Whose Space? Differences among Users and Non-Users of Social Network Sites. Journal of\\nComputer-Mediated Communication 13, 1 (10 2007), 276–297. https://doi.org/10.1111/j.1083-6101.2007.00396.x\\narXiv:http://oup.prod.sis.lan/jcmc/article-pdf/13/1/276/22317170/jjcmcom0276.pdf\\n[65] Yuzi He, Keith Burghardt, and Kristina Lerman. 2020. A Geometric Solution to Fair Representations. In Proceedings\\nof the AAAI/ACM Conference on AI, Ethics, and Society . 279–285.\\n[66] Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2018. The dataset nutrition\\nlabel: A framework to drive higher data quality standards. arXiv preprint arXiv:1805.03677 (2018).\\n[67] Ayanna Howard and Jason Borenstein. 2018. The ugly truth about ourselves and our robot creations: the problem of\\nbias and social inequity. Science and engineering ethics 24, 5 (2018), 1521–1536.\\n[68] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. 2008. Labeled faces in the wild: A database\\nforstudying face recognition in unconstrained environments.\\n[69] Lingxiao Huang and Nisheeth Vishnoi. 2019. Stable and Fair Classification. In International Conference on Machine\\nLearning . 2879–2890.\\n[70] Ben Hutchinson and Margaret Mitchell. 2019. 50 Years of Test (Un) fairness: Lessons for Machine Learning. In\\nProceedings of the Conference on Fairness, Accountability, and Transparency . ACM, 49–58.\\n[71] L. Introna and H. Nissenbaum. 2000. Defining the Web: the politics of search engines. Computer 33, 1 (Jan 2000),\\n54–62. https://doi.org/10.1109/2.816269\\n[72] Ayush Jaiswal, Yue Wu, Wael AbdAlmageed, and Premkumar Natarajan. 2018. Unsupervised Adversarial Invariance.\\narXiv:1809.10083 [cs.LG]'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 29}, page_content='54–62. https://doi.org/10.1109/2.816269\\n[72] Ayush Jaiswal, Yue Wu, Wael AbdAlmageed, and Premkumar Natarajan. 2018. Unsupervised Adversarial Invariance.\\narXiv:1809.10083 [cs.LG]\\n[73] Ray Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa. [n.d.]. Wasserstein Fair Classification.\\n([n. d.]).\\n[74] F. Kamiran and T. Calders. 2009. Classifying without discriminating. In 2009 2nd International Conference on\\nComputer, Control and Communication . 1–6. https://doi.org/10.1109/IC4.2009.4909197\\n[75] Faisal Kamiran and Toon Calders. 2010. Classification with no discrimination by preferential sampling. In Proc. 19th\\nMachine Learning Conf. Belgium and The Netherlands . Citeseer, 1–6.\\n[76] Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for classification without discrimination.\\nKnowledge and Information Systems 33, 1 (01 Oct 2012), 1–33. https://doi.org/10.1007/s10115-011-0463-8\\n[77] Faisal Kamiran and Indr ˙e Žliobait ˙e. 2013. Explainable and Non-explainable Discrimination in Classification . Springer\\nBerlin Heidelberg, Berlin, Heidelberg, 155–170. https://doi.org/10.1007/978-3-642-30487-3_8\\n[78] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012. Fairness-aware classifier with prejudice\\nremover regularizer. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases .\\nSpringer, 35–50.\\n[79] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2018. Preventing Fairness Gerrymandering: Auditing\\nand Learning for Subgroup Fairness. In International Conference on Machine Learning . 2569–2577.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 30}, page_content='A Survey on Bias and Fairness in Machine Learning 31\\n[80] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2019. An empirical study of rich subgroup fairness for\\nmachine learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency . ACM, 100–109.\\n[81] Rogier Kievit, Willem Eduard Frankenhuis, Lourens Waldorp, and Denny Borsboom. 2013. Simpson’s paradox in\\npsychological science: a practical guide. Frontiers in psychology 4 (2013), 513.\\n[82] Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard\\nSchölkopf. 2017. Avoiding discrimination through causal reasoning. In Advances in Neural Information Processing\\nSystems . 656–666.\\n[83] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2016. Inherent trade-offs in the fair determination of risk\\nscores. arXiv preprint arXiv:1609.05807 (2016).\\n[84] Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit , V ol. 5. 79–86.\\n[85] Emmanouil Krasanakis, Eleftherios Spyromitros-Xioufis, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018.\\nAdaptive Sensitive Reweighting to Mitigate Bias in Fairness-aware Classification. In Proceedings of the 2018 World\\nWide Web Conference (Lyon, France) (WWW ’18) . International World Wide Web Conferences Steering Committee,\\nRepublic and Canton of Geneva, Switzerland, 853–862. https://doi.org/10.1145/3178876.3186133\\n[86] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper\\nUijlings, Stefan Popov, Andreas Veit, et al .2017. Openimages: A public dataset for large-scale multi-label and\\nmulti-class image classification. Dataset available from https://github. com/openimages 2, 3 (2017), 2–3.\\n[87] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual Fairness. In Advances in Neural\\nInformation Processing Systems 30 , I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\\nR. Garnett (Eds.). Curran Associates, Inc., 4066–4076. http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf\\n[88] Anja Lambrecht and Catherine E Tucker. 2018. Algorithmic bias? An empirical study into apparent gender-based\\ndiscrimination in the display of STEM career ads. An Empirical Study into Apparent Gender-Based Discrimination in\\nthe Display of STEM Career Ads (March 9, 2018) (2018).\\n[89] J Larson, S Mattu, L Kirchner, and J Angwin. 2016. Compas analysis. GitHub, available at: https://github.\\ncom/propublica/compas-analysis[Google Scholar] (2016).\\n[90] Blake Lemoine, Brian Zhang, and M Mitchell. 2018. Mitigating Unwanted Biases with Adversarial Learning. (2018).\\n[91] Kristina Lerman. 2018. Computational social scientist beware: Simpson’s paradox in behavioral data. Journal of\\nComputational Social Science 1, 1 (2018), 49–58.\\n[92] Kristina Lerman and Tad Hogg. 2014. Leveraging position bias to improve peer recommendation. PLoS One 9, 6\\n(2014), e98914. http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0098914\\n[93] Kristina Lerman and Tad Hogg. 2014. Leveraging position bias to improve peer recommendation. PloS one 9, 6 (2014),\\ne98914.\\n[94] Zachary C Lipton, Alexandra Chouldechova, and Julian McAuley. 2017. Does mitigating ML’s disparate impact require\\ndisparate treatment? stat1050 (2017), 19.\\n[95] Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. 2018. Delayed Impact of Fair Machine\\nLearning. In Proceedings of the 35th International Conference on Machine Learning .\\n[96] Joshua R Loftus, Chris Russell, Matt J Kusner, and Ricardo Silva. 2018. Causal reasoning for algorithmic fairness.\\narXiv preprint arXiv:1805.05859 (2018).\\n[97] Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. 2016. THE V ARIATIONAL FAIR\\nAUTOENCODER. stat1050 (2016), 4.\\n[98] Arjun K. Manrai, Birgit H. Funke, Heidi L. Rehm, Morten S. Olesen, Bradley A. Maron, Peter Szolovits, David M.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 30}, page_content='AUTOENCODER. stat1050 (2016), 4.\\n[98] Arjun K. Manrai, Birgit H. Funke, Heidi L. Rehm, Morten S. Olesen, Bradley A. Maron, Peter Szolovits, David M.\\nMargulies, Joseph Loscalzo, and Isaac S. Kohane. 2016. Genetic Misdiagnoses and the Potential for Health Dis-\\nparities. New England Journal of Medicine 375, 7 (2016), 655–665. https://doi.org/10.1056/NEJMsa1507092\\narXiv:https://doi.org/10.1056/NEJMsa1507092 PMID: 27532831.\\n[99] Ray Marshall. 1974. The economics of racial discrimination: A survey. Journal of Economic Literature 12, 3 (1974),\\n849–871.\\n[100] Chandler May, Alex Wang, Shikha Bordia, Samuel R Bowman, and Rachel Rudinger. 2019. On measuring social\\nbiases in sentence encoders. arXiv preprint arXiv:1903.10561 (2019).\\n[101] Ninareh Mehrabi, Thamme Gowda, Fred Morstatter, Nanyun Peng, and Aram Galstyan. 2019. Man is to Person as\\nWoman is to Location: Measuring Gender Bias in Named Entity Recognition. arXiv preprint arXiv:1910.10872 (2019).\\n[102] Ninareh Mehrabi, Umang Gupta, Fred Morstatter, Greg Ver Steeg, and Aram Galstyan. 2021. Attributing Fair Decisions\\nwith Attention Interventions. arXiv preprint arXiv:2109.03952 (2021).\\n[103] Ninareh Mehrabi, Yuzhong Huang, and Fred Morstatter. 2020. Statistical Equity: A Fairness Classification Objective.\\narXiv preprint arXiv:2005.07293 (2020).\\n[104] Ninareh Mehrabi, Fred Morstatter, Nanyun Peng, and Aram Galstyan. 2019. Debiasing Community Detection: The\\nImportance of Lowly-Connected Nodes. arXiv preprint arXiv:1903.08136 (2019).'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 31}, page_content='32 Mehrabi et al.\\n[105] Ninareh Mehrabi, Pei Zhou, Fred Morstatter, Jay Pujara, Xiang Ren, and Aram Galstyan. 2021. Lawyers are Dishonest?\\nQuantifying Representational Harms in Commonsense Knowledge Resources. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta\\nCana, Dominican Republic, 5016–5033. https://doi.org/10.18653/v1/2021.emnlp-main.410\\n[106] Aditya Krishna Menon and Robert C Williamson. 2018. The cost of fairness in binary classification. In Proceedings\\nof the 1st Conference on Fairness, Accountability and Transparency (Proceedings of Machine Learning Research,\\nVol. 81) , Sorelle A. Friedler and Christo Wilson (Eds.). PMLR, New York, NY , USA, 107–118. http://proceedings.mlr.\\npress/v81/menon18a.html\\n[107] Michele Merler, Nalini Ratha, Rogerio S Feris, and John R Smith. 2019. Diversity in Faces. arXiv preprint\\narXiv:1901.10436 (2019).\\n[108] Hannah Jean Miller, Jacob Thebault-Spieker, Shuo Chang, Isaac Johnson, Loren Terveen, and Brent Hecht. 2016.\\n“Blissfully Happy” or “Ready toFight”: Varying Interpretations of Emoji. In Tenth International AAAI Conference on\\nWeb and Social Media .\\n[109] I Minchev, G Matijevic, DW Hogg, G Guiglion, M Steinmetz, F Anders, C Chiappini, M Martig, A Queiroz, and C\\nScannapieco. 2019. Yule-Simpson’s paradox in Galactic Archaeology. arXiv preprint arXiv:1902.01421 (2019).\\n[110] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,\\nInioluwa Deborah Raji, and Timnit Gebru. 2019. Model Cards for Model Reporting. In Proceedings of the Conference\\non Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* ’19) . ACM, New York, NY , USA, 220–229.\\nhttps://doi.org/10.1145/3287560.3287596\\n[111] Fred Morstatter, Jürgen Pfeffer, Huan Liu, and Kathleen M Carley. 2013. Is the sample good enough? Comparing data\\nfrom twitter’s streaming API with Twitter’s firehose. In 7th International AAAI Conference on Weblogs and Social\\nMedia, ICWSM 2013 . AAAI press.\\n[112] Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, and Greg Ver Steeg. 2018. Invariant Representations\\nwithout Adversarial Training. In Advances in Neural Information Processing Systems . 9084–9093.\\n[113] Amitabha Mukerjee, Rita Biswas, Kalyanmoy Deb, and Amrit P Mathur. 2002. Multi–objective evolutionary algorithms\\nfor the risk–return trade–off in bank loan management. International Transactions in operational research 9, 5 (2002),\\n583–597.\\n[114] David B Mustard. 2003. Reexamining criminal behavior: the importance of omitted variable bias. Review of Economics\\nand Statistics 85, 1 (2003), 205–211.\\n[115] Razieh Nabi, Daniel Malinsky, and Ilya Shpitser. 2018. Learning Optimal Fair Policies. arXiv preprint\\narXiv:1809.02244 (2018).\\n[116] Razieh Nabi and Ilya Shpitser. 2018. Fair inference on outcomes. In Thirty-Second AAAI Conference on Artificial\\nIntelligence .\\n[117] Azadeh Nematzadeh, Giovanni Luca Ciampaglia, Filippo Menczer, and Alessandro Flammini. 2017. How algorithmic\\npopularity bias hinders or promotes quality. arXiv preprint arXiv:1707.00574 (2017).\\n[118] Dong-Phuong Nguyen, Rilana Gravel, Rudolf Berend Trieschnigg, and Theo Meder. 2013. \"How old do you think\\nI am?\": A study of language and age in Twitter. In Proceedings of the Seventh International AAAI Conference on\\nWeblogs and Social Media, ICWSM 2013 . AAAI Press, 439–448. eemcs-eprint-23604.\\n[119] Anne O’Keeffe and Michael McCarthy. 2010. The Routledge handbook of corpus linguistics . Routledge.\\n[120] Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2016. Social data: Biases, methodological\\npitfalls, and ethical boundaries. (2016).\\n[121] Cathy O’Neil. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy .\\nCrown Publishing Group, New York, NY , USA.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 31}, page_content='[121] Cathy O’Neil. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy .\\nCrown Publishing Group, New York, NY , USA.\\n[122] Luca Oneto, Michele Doninini, Amon Elders, and Massimiliano Pontil. 2019. Taking advantage of multitask learning\\nfor fair classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society . 227–237.\\n[123] Osonde A Osoba and William Welser IV . 2017. An intelligence in our image: The risks of bias and errors in artificial\\nintelligence . Rand Corporation.\\n[124] Edmund S Phelps. 1972. The statistical theory of racism and sexism. The american economic review 62, 4 (1972),\\n659–661.\\n[125] Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. 2017. On Fairness and\\nCalibration. In Advances in Neural Information Processing Systems 30 , I. Guyon, U. V . Luxburg, S. Bengio,\\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc., 5680–5689. http:\\n//papers.nips.cc/paper/7151-on-fairness-and-calibration.pdf\\n[126] Marcelo OR Prates, Pedro H Avelar, and Luís C Lamb. 2018. Assessing gender bias in machine translation: a case\\nstudy with Google Translate. Neural Computing and Applications (2018), 1–19.\\n[127] Bilal Qureshi, Faisal Kamiran, Asim Karim, and Salvatore Ruggieri. 2016. Causal discrimination discovery through\\npropensity score analysis. arXiv preprint arXiv:1608.03735 (2016).'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 32}, page_content='A Survey on Bias and Fairness in Machine Learning 33\\n[128] Inioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable auditing: Investigating the impact of publicly naming\\nbiased performance results of commercial ai products.\\n[129] M Redmond. 2011. Communities and crime unnormalized data set. UCI Machine Learning Repository. In website:\\nhttp://www. ics. uci. edu/mlearn/MLRepository. html (2011).\\n[130] Willy E Rice. 1996. Race, Gender, Redlining, and the Discriminatory Access to Loans, Credit, and Insurance:\\nAn Historical and Empirical Analysis of Consumers Who Sued Lenders and Insurers in Federal and State Courts,\\n1950-1995. San Diego L. Rev. 33 (1996), 583.\\n[131] Stephanie K Riegg. 2008. Causal inference and omitted variable bias in financial aid research: Assessing solutions.\\nThe Review of Higher Education 31, 3 (2008), 329–354.\\n[132] Lauren A Rivera. 2012. Hiring as cultural matching: The case of elite professional service firms. American sociological\\nreview 77, 6 (2012), 999–1022.\\n[133] Andrea Romei and Salvatore Ruggieri. 2011. A multidisciplinary survey on discrimination analysis.\\n[134] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender Bias in Coreference\\nResolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 2 (Short Papers) . Association for Computational Linguistics, New\\nOrleans, Louisiana, 8–14. https://doi.org/10.18653/v1/N18-2002\\n[135] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, et al .2015. Imagenet large scale visual recognition challenge. International journal\\nof computer vision 115, 3 (2015), 211–252.\\n[136] Pedro Saleiro, Benedict Kuester, Abby Stevens, Ari Anisfeld, Loren Hinkson, Jesse London, and Rayid Ghani. 2018.\\nAequitas: A Bias and Fairness Audit Toolkit. arXiv preprint arXiv:1811.05577 (2018).\\n[137] Samira Samadi, Uthaipon Tantipongpipat, Jamie Morgenstern, Mohit Singh, and Santosh Vempala. 2018. The Price\\nof Fair PCA: One Extra Dimension. In Proceedings of the 32Nd International Conference on Neural Information\\nProcessing Systems (Montr&#233;al, Canada) (NIPS’18) . Curran Associates Inc., USA, 10999–11010. http://dl.acm.\\norg/citation.cfm?id=3327546.3327755\\n[138] Nripsuta Ani Saxena. 2019. Perceptions of Fairness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics,\\nand Society (Honolulu, HI, USA) (AIES ’19) . ACM, New York, NY , USA, 537–538. https://doi.org/10.1145/3306618.\\n3314314\\n[139] Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Goran Radanovic, David C Parkes, and Yang Liu. 2019. How Do\\nFairness Definitions Fare?: Examining Public Attitudes Towards Algorithmic Definitions of Fairness. In Proceedings of\\nthe 2019 AAAI/ACM Conference on AI, Ethics, and Society . ACM, 99–106.\\n[140] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. Recom-\\nmendations as Treatments: Debiasing Learning and Evaluation. In International Conference on Machine Learning .\\n1670–1679.\\n[141] Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. Fairness and\\nabstraction in sociotechnical systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency .\\nACM, 59–68.\\n[142] Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D Sculley. 2017. No Classification\\nwithout Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World. stat1050 (2017),\\n22.\\n[143] Richard Shaw and Manuel Corpas. [n.d.]. Further bias in personal genomics? ([n. d.]).\\n[144] Harini Suresh and John V Guttag. 2019. A Framework for Understanding Unintended Consequences of Machine\\nLearning. arXiv preprint arXiv:1901.10002 (2019).\\n[145] Songül Tolan, Marius Miron, Emilia Gómez, and Carlos Castillo. 2019. Why Machine Learning May Lead to'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 32}, page_content='Learning. arXiv preprint arXiv:1901.10002 (2019).\\n[145] Songül Tolan, Marius Miron, Emilia Gómez, and Carlos Castillo. 2019. Why Machine Learning May Lead to\\nUnfairness: Evidence from Risk Assessment for Juvenile Justice in Catalonia. (2019).\\n[146] Zeynep Tufekci. 2014. Big questions for social media big data: Representativeness, validity and other methodological\\npitfalls. In Eighth International AAAI Conference on Weblogs and Social Media .\\n[147] Berk Ustun, Yang Liu, and David Parkes. 2019. Fairness without Harm: Decoupled Classifiers with Preference\\nGuarantees. In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine\\nLearning Research, Vol. 97) , Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, Long Beach, California,\\nUSA, 6373–6382. http://proceedings.mlr.press/v97/ustun19a.html\\n[148] Eva Vanmassenhove, Christian Hardmeier, and Andy Way. 2018. Getting gender right in neural machine translation. In\\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 3003–3008.\\n[149] Sahil Verma and Julia Rubin. 2018. Fairness definitions explained. In 2018 IEEE/ACM International Workshop on\\nSoftware Fairness (FairWare) . IEEE, 1–7.\\n[150] Selwyn Vickers, Mona Fouad, and Moon S Chen Jr. 2014. Enhancing Minority Participation in Clinical Trials\\n(EMPaCT): laying the groundwork for improving minority clinical trial accrual. Cancer 120 (2014), vi–vii.'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 33}, page_content='34 Mehrabi et al.\\n[151] Ting Wang and Dashun Wang. 2014. Why Amazon’s ratings might mislead you: The story of herding effects. Big data\\n2, 4 (2014), 196–204.\\n[152] Steven L Willborn. 1984. The disparate impact model of discrimination: Theory and limits. Am. UL Rev. 34 (1984),\\n799.\\n[153] Christo Wilson, Bryce Boe, Alessandra Sala, Krishna PN Puttaswamy, and Ben Y Zhao. 2009. User interactions in\\nsocial networks and their implications. In Proceedings of the 4th ACM European conference on Computer systems .\\nAcm, 205–218.\\n[154] Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. 2017. Learning non-discriminatory\\npredictors. arXiv preprint arXiv:1702.06081 (2017).\\n[155] Yongkai Wu, Lu Zhang, and Xintao Wu. 2018. Fairness-aware Classification: Criterion, Convexity, and Bounds.\\narXiv:1809.04737 [cs.LG]\\n[156] Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu. 2018. Fairgan: Fairness-aware generative adversarial networks.\\nIn2018 IEEE International Conference on Big Data (Big Data) . IEEE, 570–575.\\n[157] Irene Y Chen, Peter Szolovits, and Marzyeh Ghassemi. 2019. Can AI Help Reduce Disparities in General Medical and\\nMental Health Care? AMA journal of ethics 21 (02 2019), E167–179. https://doi.org/10.1001/amajethics.2019.167\\n[158] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. 2017. Fairness beyond\\ndisparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the\\n26th international conference on world wide web . 1171–1180.\\n[159] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. 2015. Fairness constraints:\\nMechanisms for fair classification. arXiv preprint arXiv:1507.05259 (2015).\\n[160] Lu Zhang and Xintao Wu. 2017. Anti-discrimination learning: a causal modeling-based framework. International\\nJournal of Data Science and Analytics 4, 1 (01 Aug 2017), 1–16. https://doi.org/10.1007/s41060-017-0058-x\\n[161] Lu Zhang, Yongkai Wu, and Xintao Wu. 2016. On Discrimination Discovery Using Causal Networks. In Social,\\nCultural, and Behavioral Modeling , Kevin S. Xu, David Reitter, Dongwon Lee, and Nathaniel Osgood (Eds.). Springer\\nInternational Publishing, Cham, 83–93.\\n[162] Lu Zhang, Yongkai Wu, and Xintao Wu. 2016. Situation Testing-based Discrimination Discovery: A Causal Inference\\nApproach. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (New York, New\\nYork, USA) (IJCAI’16) . AAAI Press, 2718–2724. http://dl.acm.org/citation.cfm?id=3060832.3061001\\n[163] Lu Zhang, Yongkai Wu, and Xintao Wu. 2017. Achieving non-discrimination in data release. In Proceedings of the\\n23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM, 1335–1344.\\n[164] Lu Zhang, Yongkai Wu, and Xintao Wu. 2017. A Causal Framework for Discovering and Removing Direct and Indirect\\nDiscrimination. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17 .\\n3929–3935. https://doi.org/10.24963/ijcai.2017/549\\n[165] L. Zhang, Y . Wu, and X. Wu. 2018. Causal Modeling-Based Discrimination Discovery and Removal: Criteria, Bounds,\\nand Algorithms. IEEE Transactions on Knowledge and Data Engineering (2018), 1–1. https://doi.org/10.1109/TKDE.\\n2018.2872988\\n[166] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. 2019. Gender Bias\\nin Contextualized Word Embeddings. In Proceedings of the 2019 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) .\\n629–634.\\n[167] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men Also Like Shopping:\\nReducing Gender Bias Amplification using Corpus-level Constraints. In Proceedings of the 2017 Conference on\\nEmpirical Methods in Natural Language Processing .'),\n",
       " Document(metadata={'source': 'BiasFairnessSurvey.pdf', 'page': 33}, page_content='Reducing Gender Bias Amplification using Corpus-level Constraints. In Proceedings of the 2017 Conference on\\nEmpirical Methods in Natural Language Processing .\\n[168] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018. Gender Bias in Coreference\\nResolution: Evaluation and Debiasing Methods. arXiv:1804.06876 [cs.CL]\\n[169] Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-Wei Chang. 2018. Learning Gender-Neutral Word Embeddings.\\nInProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 4847–4853.\\n[170] James Zou and Londa Schiebinger. 2018. AI can be sexist and racist it’s time to make it fair. Nature Publishing Group.')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testingopenai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
